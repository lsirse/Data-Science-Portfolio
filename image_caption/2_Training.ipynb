{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** While choosing the parameters I look at the papers provided in the links as well as used my own intuition. I chose the batch_size and vocab_threshold to be rather small, so that the training process is steady and stable. The size of embedding around 256 seems to be big enough, so that the training process does not take too long and information is not lost at the same time. In addition, 512 features in the hidden state seems to be large enough for an optimal performance, since our goal was not perfection. \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** I chose to keep the transform_train method as it is, because it was using the most popular preprocessing techniques, therefore, it is a reliable choice. Feature augmentation (such as RandomVerticalFlipping or RandomHorizontalCrop) would not be very relevant here, because standing person is something different than a person standing upside down. The image size of 224x224 is most relevant for resnet pre-trained weights.\n",
    "\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** I selected to train the decoder weights and only the embedding layer of the encoder. The reason for this is that pre-trained CNN Resnet already has a lot of useful weights. Therefore, we can expect it to give reasonable results while we can also save some training size by reducing the number of parameters.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** I chose Adamax optimizer, because it is considered a superior version of Adam algorithm, especially in the cases when model has embeddings. (https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.90s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 887/414113 [00:00<01:32, 4453.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:36<00:00, 4286.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 10          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = False    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adamax(params)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 4.5214, Perplexity: 91.9626\n",
      "Epoch [1/3], Step [200/41412], Loss: 4.0597, Perplexity: 57.95882\n",
      "Epoch [1/3], Step [300/41412], Loss: 4.2015, Perplexity: 66.78486\n",
      "Epoch [1/3], Step [400/41412], Loss: 3.2306, Perplexity: 25.29395\n",
      "Epoch [1/3], Step [500/41412], Loss: 4.0374, Perplexity: 56.68066\n",
      "Epoch [1/3], Step [600/41412], Loss: 3.2464, Perplexity: 25.69825\n",
      "Epoch [1/3], Step [700/41412], Loss: 3.5822, Perplexity: 35.9524\n",
      "Epoch [1/3], Step [800/41412], Loss: 4.0962, Perplexity: 60.10892\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.1970, Perplexity: 24.45947\n",
      "Epoch [1/3], Step [1000/41412], Loss: 3.4185, Perplexity: 30.5224\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.4976, Perplexity: 33.03461\n",
      "Epoch [1/3], Step [1200/41412], Loss: 3.6770, Perplexity: 39.5277\n",
      "Epoch [1/3], Step [1300/41412], Loss: 3.4396, Perplexity: 31.1737\n",
      "Epoch [1/3], Step [1400/41412], Loss: 2.8990, Perplexity: 18.1562\n",
      "Epoch [1/3], Step [1500/41412], Loss: 3.5555, Perplexity: 35.0066\n",
      "Epoch [1/3], Step [1600/41412], Loss: 3.2785, Perplexity: 26.5372\n",
      "Epoch [1/3], Step [1700/41412], Loss: 3.1337, Perplexity: 22.95987\n",
      "Epoch [1/3], Step [1800/41412], Loss: 3.6201, Perplexity: 37.3430\n",
      "Epoch [1/3], Step [1900/41412], Loss: 2.5414, Perplexity: 12.6977\n",
      "Epoch [1/3], Step [2000/41412], Loss: 3.9075, Perplexity: 49.77264\n",
      "Epoch [1/3], Step [2100/41412], Loss: 2.5815, Perplexity: 13.2171\n",
      "Epoch [1/3], Step [2200/41412], Loss: 2.7445, Perplexity: 15.5574\n",
      "Epoch [1/3], Step [2300/41412], Loss: 2.7735, Perplexity: 16.0154\n",
      "Epoch [1/3], Step [2400/41412], Loss: 2.8205, Perplexity: 16.7845\n",
      "Epoch [1/3], Step [2500/41412], Loss: 2.8926, Perplexity: 18.03977\n",
      "Epoch [1/3], Step [2600/41412], Loss: 2.9690, Perplexity: 19.4726\n",
      "Epoch [1/3], Step [2700/41412], Loss: 3.1225, Perplexity: 22.7029\n",
      "Epoch [1/3], Step [2800/41412], Loss: 2.3248, Perplexity: 10.2246\n",
      "Epoch [1/3], Step [2900/41412], Loss: 3.1753, Perplexity: 23.9339\n",
      "Epoch [1/3], Step [3000/41412], Loss: 3.0946, Perplexity: 22.0789\n",
      "Epoch [1/3], Step [3100/41412], Loss: 3.3987, Perplexity: 29.9241\n",
      "Epoch [1/3], Step [3200/41412], Loss: 2.9113, Perplexity: 18.3799\n",
      "Epoch [1/3], Step [3300/41412], Loss: 2.5550, Perplexity: 12.8712\n",
      "Epoch [1/3], Step [3400/41412], Loss: 2.8740, Perplexity: 17.7082\n",
      "Epoch [1/3], Step [3500/41412], Loss: 3.0905, Perplexity: 21.9884\n",
      "Epoch [1/3], Step [3600/41412], Loss: 3.6602, Perplexity: 38.8705\n",
      "Epoch [1/3], Step [3700/41412], Loss: 3.1532, Perplexity: 23.41184\n",
      "Epoch [1/3], Step [3800/41412], Loss: 3.2517, Perplexity: 25.8337\n",
      "Epoch [1/3], Step [3900/41412], Loss: 2.8531, Perplexity: 17.3407\n",
      "Epoch [1/3], Step [4000/41412], Loss: 3.0850, Perplexity: 21.8676\n",
      "Epoch [1/3], Step [4100/41412], Loss: 3.1522, Perplexity: 23.3881\n",
      "Epoch [1/3], Step [4200/41412], Loss: 3.1035, Perplexity: 22.2752\n",
      "Epoch [1/3], Step [4300/41412], Loss: 2.9790, Perplexity: 19.6680\n",
      "Epoch [1/3], Step [4400/41412], Loss: 3.2355, Perplexity: 25.41873\n",
      "Epoch [1/3], Step [4500/41412], Loss: 3.4069, Perplexity: 30.1706\n",
      "Epoch [1/3], Step [4600/41412], Loss: 3.0072, Perplexity: 20.2302\n",
      "Epoch [1/3], Step [4700/41412], Loss: 2.8117, Perplexity: 16.6375\n",
      "Epoch [1/3], Step [4800/41412], Loss: 2.4157, Perplexity: 11.1972\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.5776, Perplexity: 13.1657\n",
      "Epoch [1/3], Step [5000/41412], Loss: 3.5745, Perplexity: 35.6782\n",
      "Epoch [1/3], Step [5100/41412], Loss: 3.5014, Perplexity: 33.1604\n",
      "Epoch [1/3], Step [5200/41412], Loss: 2.3536, Perplexity: 10.5235\n",
      "Epoch [1/3], Step [5300/41412], Loss: 2.9989, Perplexity: 20.0626\n",
      "Epoch [1/3], Step [5400/41412], Loss: 3.0296, Perplexity: 20.6897\n",
      "Epoch [1/3], Step [5500/41412], Loss: 2.4082, Perplexity: 11.1138\n",
      "Epoch [1/3], Step [5600/41412], Loss: 2.6045, Perplexity: 13.5241\n",
      "Epoch [1/3], Step [5700/41412], Loss: 3.4311, Perplexity: 30.9114\n",
      "Epoch [1/3], Step [5800/41412], Loss: 2.5605, Perplexity: 12.9425\n",
      "Epoch [1/3], Step [5900/41412], Loss: 3.1963, Perplexity: 24.4424\n",
      "Epoch [1/3], Step [6000/41412], Loss: 2.1129, Perplexity: 8.27231\n",
      "Epoch [1/3], Step [6100/41412], Loss: 2.8168, Perplexity: 16.72340\n",
      "Epoch [1/3], Step [6200/41412], Loss: 4.4657, Perplexity: 86.9861\n",
      "Epoch [1/3], Step [6300/41412], Loss: 3.0506, Perplexity: 21.1285\n",
      "Epoch [1/3], Step [6400/41412], Loss: 3.0217, Perplexity: 20.5262\n",
      "Epoch [1/3], Step [6500/41412], Loss: 3.5729, Perplexity: 35.6183\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.4383, Perplexity: 11.4531\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.9980, Perplexity: 20.0460\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.3296, Perplexity: 10.2743\n",
      "Epoch [1/3], Step [6900/41412], Loss: 2.7627, Perplexity: 15.8430\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.7183, Perplexity: 15.1549\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.8356, Perplexity: 17.0406\n",
      "Epoch [1/3], Step [7200/41412], Loss: 2.6689, Perplexity: 14.4234\n",
      "Epoch [1/3], Step [7300/41412], Loss: 2.5265, Perplexity: 12.5093\n",
      "Epoch [1/3], Step [7400/41412], Loss: 2.8166, Perplexity: 16.7199\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.2698, Perplexity: 9.67729\n",
      "Epoch [1/3], Step [7600/41412], Loss: 2.8016, Perplexity: 16.4711\n",
      "Epoch [1/3], Step [7700/41412], Loss: 2.8638, Perplexity: 17.5277\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.8080, Perplexity: 16.5761\n",
      "Epoch [1/3], Step [7900/41412], Loss: 2.5557, Perplexity: 12.8801\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.7901, Perplexity: 16.2831\n",
      "Epoch [1/3], Step [8100/41412], Loss: 2.5022, Perplexity: 12.2090\n",
      "Epoch [1/3], Step [8200/41412], Loss: 2.7292, Perplexity: 15.3202\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.2171, Perplexity: 9.18099\n",
      "Epoch [1/3], Step [8400/41412], Loss: 2.9204, Perplexity: 18.5483\n",
      "Epoch [1/3], Step [8500/41412], Loss: 3.0713, Perplexity: 21.5694\n",
      "Epoch [1/3], Step [8600/41412], Loss: 3.2226, Perplexity: 25.0934\n",
      "Epoch [1/3], Step [8700/41412], Loss: 2.3249, Perplexity: 10.2257\n",
      "Epoch [1/3], Step [8800/41412], Loss: 2.8247, Perplexity: 16.8562\n",
      "Epoch [1/3], Step [8900/41412], Loss: 2.3044, Perplexity: 10.0186\n",
      "Epoch [1/3], Step [9000/41412], Loss: 3.2149, Perplexity: 24.9014\n",
      "Epoch [1/3], Step [9100/41412], Loss: 3.1603, Perplexity: 23.5772\n",
      "Epoch [1/3], Step [9200/41412], Loss: 2.4664, Perplexity: 11.7796\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.6126, Perplexity: 13.6339\n",
      "Epoch [1/3], Step [9400/41412], Loss: 3.1011, Perplexity: 22.2219\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.3727, Perplexity: 10.7268\n",
      "Epoch [1/3], Step [9600/41412], Loss: 2.7189, Perplexity: 15.1632\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.3474, Perplexity: 10.4579\n",
      "Epoch [1/3], Step [9800/41412], Loss: 2.4321, Perplexity: 11.3830\n",
      "Epoch [1/3], Step [9900/41412], Loss: 2.5420, Perplexity: 12.70548\n",
      "Epoch [1/3], Step [10000/41412], Loss: 2.8718, Perplexity: 17.6689\n",
      "Epoch [1/3], Step [10100/41412], Loss: 2.4707, Perplexity: 11.8304\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.6958, Perplexity: 14.8180\n",
      "Epoch [1/3], Step [10300/41412], Loss: 3.0023, Perplexity: 20.13083\n",
      "Epoch [1/3], Step [10400/41412], Loss: 3.3356, Perplexity: 28.0943\n",
      "Epoch [1/3], Step [10500/41412], Loss: 2.5953, Perplexity: 13.4008\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.2165, Perplexity: 9.17553\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.6652, Perplexity: 14.3708\n",
      "Epoch [1/3], Step [10800/41412], Loss: 2.4919, Perplexity: 12.0842\n",
      "Epoch [1/3], Step [10900/41412], Loss: 2.6779, Perplexity: 14.5539\n",
      "Epoch [1/3], Step [11000/41412], Loss: 2.5005, Perplexity: 12.1889\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.3811, Perplexity: 10.8171\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.5342, Perplexity: 12.6058\n",
      "Epoch [1/3], Step [11300/41412], Loss: 2.6638, Perplexity: 14.3505\n",
      "Epoch [1/3], Step [11400/41412], Loss: 2.7258, Perplexity: 15.2680\n",
      "Epoch [1/3], Step [11500/41412], Loss: 2.8061, Perplexity: 16.5460\n",
      "Epoch [1/3], Step [11600/41412], Loss: 3.0452, Perplexity: 21.0133\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.3173, Perplexity: 10.1479\n",
      "Epoch [1/3], Step [11800/41412], Loss: 1.8169, Perplexity: 6.15292\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.1408, Perplexity: 8.50654\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.3657, Perplexity: 10.6517\n",
      "Epoch [1/3], Step [12100/41412], Loss: 3.0797, Perplexity: 21.7521\n",
      "Epoch [1/3], Step [12200/41412], Loss: 3.2139, Perplexity: 24.8755\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.2683, Perplexity: 9.66321\n",
      "Epoch [1/3], Step [12400/41412], Loss: 2.0996, Perplexity: 8.163317\n",
      "Epoch [1/3], Step [12500/41412], Loss: 3.1318, Perplexity: 22.9156\n",
      "Epoch [1/3], Step [12600/41412], Loss: 2.2552, Perplexity: 9.53723\n",
      "Epoch [1/3], Step [12700/41412], Loss: 2.1252, Perplexity: 8.37473\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.5133, Perplexity: 12.3455\n",
      "Epoch [1/3], Step [12900/41412], Loss: 2.6686, Perplexity: 14.41939\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.5256, Perplexity: 12.4985\n",
      "Epoch [1/3], Step [13100/41412], Loss: 2.5359, Perplexity: 12.6281\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.5313, Perplexity: 12.5697\n",
      "Epoch [1/3], Step [13300/41412], Loss: 3.5769, Perplexity: 35.7628\n",
      "Epoch [1/3], Step [13400/41412], Loss: 2.2231, Perplexity: 9.23632\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.4614, Perplexity: 11.7211\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.6970, Perplexity: 14.8356\n",
      "Epoch [1/3], Step [13700/41412], Loss: 2.3392, Perplexity: 10.37346\n",
      "Epoch [1/3], Step [13800/41412], Loss: 2.4380, Perplexity: 11.4505\n",
      "Epoch [1/3], Step [13900/41412], Loss: 2.8249, Perplexity: 16.8588\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.4759, Perplexity: 11.8921\n",
      "Epoch [1/3], Step [14100/41412], Loss: 2.2049, Perplexity: 9.06908\n",
      "Epoch [1/3], Step [14200/41412], Loss: 1.9794, Perplexity: 7.23831\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.6984, Perplexity: 14.8558\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.2742, Perplexity: 9.71975\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.4457, Perplexity: 11.5384\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.3644, Perplexity: 10.6380\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.1252, Perplexity: 8.37499\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.0780, Perplexity: 7.98823\n",
      "Epoch [1/3], Step [14900/41412], Loss: 3.1282, Perplexity: 22.8319\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.2526, Perplexity: 9.51230\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.7274, Perplexity: 15.2931\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.0565, Perplexity: 7.81833\n",
      "Epoch [1/3], Step [15300/41412], Loss: 2.5349, Perplexity: 12.6151\n",
      "Epoch [1/3], Step [15400/41412], Loss: 2.0633, Perplexity: 7.87164\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.3479, Perplexity: 10.4633\n",
      "Epoch [1/3], Step [15600/41412], Loss: 2.4381, Perplexity: 11.4511\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.2140, Perplexity: 9.15191\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.0306, Perplexity: 7.61853\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.9277, Perplexity: 18.6848\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.2810, Perplexity: 9.78678\n",
      "Epoch [1/3], Step [16100/41412], Loss: 3.0194, Perplexity: 20.4796\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.8018, Perplexity: 16.4735\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.5690, Perplexity: 13.0529\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.2038, Perplexity: 9.059004\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.8525, Perplexity: 17.3302\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.3928, Perplexity: 10.9440\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.0989, Perplexity: 8.15716\n",
      "Epoch [1/3], Step [16800/41412], Loss: 2.8158, Perplexity: 16.7062\n",
      "Epoch [1/3], Step [16900/41412], Loss: 3.0242, Perplexity: 20.5776\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.5646, Perplexity: 12.9956\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.5191, Perplexity: 12.4180\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.5954, Perplexity: 13.4023\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.2265, Perplexity: 9.26739\n",
      "Epoch [1/3], Step [17400/41412], Loss: 2.3341, Perplexity: 10.3197\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.3427, Perplexity: 10.4096\n",
      "Epoch [1/3], Step [17600/41412], Loss: 2.6922, Perplexity: 14.7640\n",
      "Epoch [1/3], Step [17700/41412], Loss: 3.1913, Perplexity: 24.3191\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.5795, Perplexity: 13.1902\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.3618, Perplexity: 10.6100\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.8711, Perplexity: 17.6558\n",
      "Epoch [1/3], Step [18100/41412], Loss: 2.6373, Perplexity: 13.9754\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.8588, Perplexity: 17.4400\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.7586, Perplexity: 15.7778\n",
      "Epoch [1/3], Step [18400/41412], Loss: 2.8946, Perplexity: 18.0764\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.3798, Perplexity: 10.8024\n",
      "Epoch [1/3], Step [18600/41412], Loss: 2.9326, Perplexity: 18.7766\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.1240, Perplexity: 8.36454\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.6431, Perplexity: 14.0574\n",
      "Epoch [1/3], Step [18900/41412], Loss: 2.8464, Perplexity: 17.2252\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.8024, Perplexity: 16.4839\n",
      "Epoch [1/3], Step [19100/41412], Loss: 1.9263, Perplexity: 6.86390\n",
      "Epoch [1/3], Step [19200/41412], Loss: 2.7224, Perplexity: 15.2162\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.1448, Perplexity: 8.54078\n",
      "Epoch [1/3], Step [19400/41412], Loss: 2.2320, Perplexity: 9.31820\n",
      "Epoch [1/3], Step [19500/41412], Loss: 2.9678, Perplexity: 19.4487\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.5529, Perplexity: 12.8439\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.5618, Perplexity: 12.9590\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.7874, Perplexity: 16.2384\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.1033, Perplexity: 8.19303\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.0961, Perplexity: 8.134662\n",
      "Epoch [1/3], Step [20100/41412], Loss: 2.8210, Perplexity: 16.7938\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.8468, Perplexity: 17.2317\n",
      "Epoch [1/3], Step [20300/41412], Loss: 3.0174, Perplexity: 20.4387\n",
      "Epoch [1/3], Step [20400/41412], Loss: 1.9840, Perplexity: 7.27157\n",
      "Epoch [1/3], Step [20500/41412], Loss: 5.2386, Perplexity: 188.4146\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.1175, Perplexity: 8.31026\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.0455, Perplexity: 7.73302\n",
      "Epoch [1/3], Step [20800/41412], Loss: 2.4608, Perplexity: 11.7145\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.2218, Perplexity: 9.22351\n",
      "Epoch [1/3], Step [21000/41412], Loss: 2.3038, Perplexity: 10.0123\n",
      "Epoch [1/3], Step [21100/41412], Loss: 2.7596, Perplexity: 15.7933\n",
      "Epoch [1/3], Step [21200/41412], Loss: 1.8131, Perplexity: 6.12969\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.7843, Perplexity: 16.1888\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.6473, Perplexity: 14.1164\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.9902, Perplexity: 19.8905\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.3088, Perplexity: 10.0628\n",
      "Epoch [1/3], Step [21700/41412], Loss: 2.2938, Perplexity: 9.91225\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.4446, Perplexity: 11.5254\n",
      "Epoch [1/3], Step [21900/41412], Loss: 2.2977, Perplexity: 9.95099\n",
      "Epoch [1/3], Step [22000/41412], Loss: 2.2865, Perplexity: 9.84084\n",
      "Epoch [1/3], Step [22100/41412], Loss: 1.9151, Perplexity: 6.78783\n",
      "Epoch [1/3], Step [22200/41412], Loss: 3.2078, Perplexity: 24.7244\n",
      "Epoch [1/3], Step [22300/41412], Loss: 1.8499, Perplexity: 6.35958\n",
      "Epoch [1/3], Step [22400/41412], Loss: 2.7630, Perplexity: 15.8474\n",
      "Epoch [1/3], Step [22500/41412], Loss: 2.2544, Perplexity: 9.52973\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.8341, Perplexity: 17.0154\n",
      "Epoch [1/3], Step [22700/41412], Loss: 1.9900, Perplexity: 7.31533\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.5948, Perplexity: 13.3938\n",
      "Epoch [1/3], Step [22900/41412], Loss: 1.9748, Perplexity: 7.20521\n",
      "Epoch [1/3], Step [23000/41412], Loss: 3.4304, Perplexity: 30.89029\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.1795, Perplexity: 8.84227\n",
      "Epoch [1/3], Step [23200/41412], Loss: 3.1295, Perplexity: 22.8623\n",
      "Epoch [1/3], Step [23300/41412], Loss: 2.6039, Perplexity: 13.5163\n",
      "Epoch [1/3], Step [23400/41412], Loss: 2.5783, Perplexity: 13.1748\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.6773, Perplexity: 14.5462\n",
      "Epoch [1/3], Step [23600/41412], Loss: 1.8906, Perplexity: 6.62331\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.5170, Perplexity: 12.3914\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.2962, Perplexity: 9.93652\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.4900, Perplexity: 12.0609\n",
      "Epoch [1/3], Step [24000/41412], Loss: 2.8227, Perplexity: 16.8226\n",
      "Epoch [1/3], Step [24100/41412], Loss: 1.8414, Perplexity: 6.30566\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.7052, Perplexity: 14.9574\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.4977, Perplexity: 12.1541\n",
      "Epoch [1/3], Step [24400/41412], Loss: 1.8059, Perplexity: 6.08551\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.7716, Perplexity: 15.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/41412], Loss: 2.5717, Perplexity: 13.0885\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.2404, Perplexity: 9.39684\n",
      "Epoch [1/3], Step [24800/41412], Loss: 2.7673, Perplexity: 15.9153\n",
      "Epoch [1/3], Step [24900/41412], Loss: 3.1234, Perplexity: 22.7230\n",
      "Epoch [1/3], Step [25000/41412], Loss: 1.9501, Perplexity: 7.02916\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.7139, Perplexity: 15.0879\n",
      "Epoch [1/3], Step [25200/41412], Loss: 2.6800, Perplexity: 14.5847\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.4455, Perplexity: 11.5362\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.2269, Perplexity: 9.27084\n",
      "Epoch [1/3], Step [25500/41412], Loss: 2.5771, Perplexity: 13.15841\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.2757, Perplexity: 9.73500\n",
      "Epoch [1/3], Step [25700/41412], Loss: 2.3480, Perplexity: 10.4647\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.2943, Perplexity: 9.91797\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.1913, Perplexity: 8.94735\n",
      "Epoch [1/3], Step [26000/41412], Loss: 2.5850, Perplexity: 13.2639\n",
      "Epoch [1/3], Step [26100/41412], Loss: 1.8231, Perplexity: 6.19122\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.0617, Perplexity: 7.85954\n",
      "Epoch [1/3], Step [26300/41412], Loss: 2.5326, Perplexity: 12.5860\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.3495, Perplexity: 10.4804\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.5300, Perplexity: 12.5530\n",
      "Epoch [1/3], Step [26600/41412], Loss: 2.3399, Perplexity: 10.3801\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.3479, Perplexity: 10.4636\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.2643, Perplexity: 9.62477\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.3576, Perplexity: 10.5657\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.0072, Perplexity: 7.44257\n",
      "Epoch [1/3], Step [27100/41412], Loss: 2.2195, Perplexity: 9.20287\n",
      "Epoch [1/3], Step [27200/41412], Loss: 2.1916, Perplexity: 8.94938\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.1142, Perplexity: 8.28297\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.0272, Perplexity: 7.59317\n",
      "Epoch [1/3], Step [27500/41412], Loss: 2.9848, Perplexity: 19.7828\n",
      "Epoch [1/3], Step [27600/41412], Loss: 2.5287, Perplexity: 12.5366\n",
      "Epoch [1/3], Step [27700/41412], Loss: 2.2893, Perplexity: 9.86787\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.1305, Perplexity: 8.41918\n",
      "Epoch [1/3], Step [27900/41412], Loss: 3.0230, Perplexity: 20.5524\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.5685, Perplexity: 13.0461\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.5277, Perplexity: 12.5241\n",
      "Epoch [1/3], Step [28200/41412], Loss: 2.4972, Perplexity: 12.1483\n",
      "Epoch [1/3], Step [28300/41412], Loss: 2.4251, Perplexity: 11.3039\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.2579, Perplexity: 9.56298\n",
      "Epoch [1/3], Step [28500/41412], Loss: 3.0280, Perplexity: 20.6549\n",
      "Epoch [1/3], Step [28600/41412], Loss: 2.0496, Perplexity: 7.76502\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.7310, Perplexity: 15.3486\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.3487, Perplexity: 10.4723\n",
      "Epoch [1/3], Step [28900/41412], Loss: 2.4393, Perplexity: 11.4644\n",
      "Epoch [1/3], Step [29000/41412], Loss: 3.5664, Perplexity: 35.3902\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.2193, Perplexity: 9.20075\n",
      "Epoch [1/3], Step [29200/41412], Loss: 2.8690, Perplexity: 17.6200\n",
      "Epoch [1/3], Step [29300/41412], Loss: 2.1361, Perplexity: 8.46630\n",
      "Epoch [1/3], Step [29400/41412], Loss: 2.0798, Perplexity: 8.00281\n",
      "Epoch [1/3], Step [29500/41412], Loss: 2.1446, Perplexity: 8.53889\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.4019, Perplexity: 11.0445\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.2935, Perplexity: 9.90930\n",
      "Epoch [1/3], Step [29800/41412], Loss: 1.8720, Perplexity: 6.50124\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.2647, Perplexity: 9.62864\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.4027, Perplexity: 11.0529\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.4406, Perplexity: 11.4797\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.2135, Perplexity: 9.14818\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.4660, Perplexity: 11.7755\n",
      "Epoch [1/3], Step [30400/41412], Loss: 1.7812, Perplexity: 5.93729\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.6054, Perplexity: 13.5366\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.4655, Perplexity: 11.7699\n",
      "Epoch [1/3], Step [30700/41412], Loss: 1.4564, Perplexity: 4.29064\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.6220, Perplexity: 13.7633\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.1075, Perplexity: 8.22773\n",
      "Epoch [1/3], Step [31000/41412], Loss: 2.3405, Perplexity: 10.3868\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.7847, Perplexity: 16.1957\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.1433, Perplexity: 8.52754\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.2757, Perplexity: 9.73455\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.4046, Perplexity: 11.0741\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.2733, Perplexity: 9.71153\n",
      "Epoch [1/3], Step [31600/41412], Loss: 2.0524, Perplexity: 7.78694\n",
      "Epoch [1/3], Step [31700/41412], Loss: 2.0287, Perplexity: 7.60423\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.3050, Perplexity: 10.0237\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.5502, Perplexity: 12.8092\n",
      "Epoch [1/3], Step [32000/41412], Loss: 1.8979, Perplexity: 6.67221\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.6980, Perplexity: 14.8498\n",
      "Epoch [1/3], Step [32200/41412], Loss: 2.9628, Perplexity: 19.3526\n",
      "Epoch [1/3], Step [32300/41412], Loss: 1.5285, Perplexity: 4.61121\n",
      "Epoch [1/3], Step [32400/41412], Loss: 3.1430, Perplexity: 23.1737\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.3776, Perplexity: 10.7793\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.4674, Perplexity: 11.7920\n",
      "Epoch [1/3], Step [32700/41412], Loss: 1.9943, Perplexity: 7.34704\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.6989, Perplexity: 14.8632\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.6023, Perplexity: 13.4943\n",
      "Epoch [1/3], Step [33000/41412], Loss: 2.4878, Perplexity: 12.0350\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.0441, Perplexity: 7.72210\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.0741, Perplexity: 7.95739\n",
      "Epoch [1/3], Step [33300/41412], Loss: 1.8732, Perplexity: 6.50930\n",
      "Epoch [1/3], Step [33400/41412], Loss: 2.5385, Perplexity: 12.6610\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.6219, Perplexity: 13.7618\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.5319, Perplexity: 12.5773\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.5758, Perplexity: 13.1420\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.4843, Perplexity: 11.9927\n",
      "Epoch [1/3], Step [33900/41412], Loss: 2.4860, Perplexity: 12.0136\n",
      "Epoch [1/3], Step [34000/41412], Loss: 1.8887, Perplexity: 6.61075\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.5654, Perplexity: 13.0055\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.7484, Perplexity: 15.6177\n",
      "Epoch [1/3], Step [34300/41412], Loss: 2.3576, Perplexity: 10.5653\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.3863, Perplexity: 10.8733\n",
      "Epoch [1/3], Step [34500/41412], Loss: 1.9503, Perplexity: 7.03090\n",
      "Epoch [1/3], Step [34600/41412], Loss: 1.9784, Perplexity: 7.23150\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.2419, Perplexity: 9.41162\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.1305, Perplexity: 8.41886\n",
      "Epoch [1/3], Step [34900/41412], Loss: 2.2577, Perplexity: 9.56130\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.3398, Perplexity: 10.3790\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.3393, Perplexity: 10.3738\n",
      "Epoch [1/3], Step [35200/41412], Loss: 1.9396, Perplexity: 6.95617\n",
      "Epoch [1/3], Step [35300/41412], Loss: 2.2212, Perplexity: 9.21797\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.0456, Perplexity: 7.73354\n",
      "Epoch [1/3], Step [35500/41412], Loss: 2.1593, Perplexity: 8.665324\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.8586, Perplexity: 17.4373\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.2933, Perplexity: 9.907819\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.1332, Perplexity: 8.44195\n",
      "Epoch [1/3], Step [35900/41412], Loss: 3.1566, Perplexity: 23.4913\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.7742, Perplexity: 16.0262\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.1372, Perplexity: 8.47558\n",
      "Epoch [1/3], Step [36200/41412], Loss: 2.2951, Perplexity: 9.92596\n",
      "Epoch [1/3], Step [36300/41412], Loss: 2.3106, Perplexity: 10.0801\n",
      "Epoch [1/3], Step [36400/41412], Loss: 2.7330, Perplexity: 15.3793\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.4175, Perplexity: 11.2174\n",
      "Epoch [1/3], Step [36600/41412], Loss: 2.1995, Perplexity: 9.02054\n",
      "Epoch [1/3], Step [36700/41412], Loss: 2.6316, Perplexity: 13.8961\n",
      "Epoch [1/3], Step [36800/41412], Loss: 2.4953, Perplexity: 12.1248\n",
      "Epoch [1/3], Step [36900/41412], Loss: 2.2293, Perplexity: 9.29342\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.2010, Perplexity: 9.03434\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.7348, Perplexity: 15.4068\n",
      "Epoch [1/3], Step [37200/41412], Loss: 2.0972, Perplexity: 8.14314\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.0549, Perplexity: 7.80610\n",
      "Epoch [1/3], Step [37400/41412], Loss: 1.9859, Perplexity: 7.28539\n",
      "Epoch [1/3], Step [37500/41412], Loss: 2.7932, Perplexity: 16.3331\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.0724, Perplexity: 7.94382\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.1779, Perplexity: 8.82800\n",
      "Epoch [1/3], Step [37800/41412], Loss: 1.9940, Perplexity: 7.34514\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.9778, Perplexity: 19.64448\n",
      "Epoch [1/3], Step [38000/41412], Loss: 2.8397, Perplexity: 17.1099\n",
      "Epoch [1/3], Step [38100/41412], Loss: 1.7540, Perplexity: 5.77749\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.1320, Perplexity: 8.43205\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.3960, Perplexity: 10.9793\n",
      "Epoch [1/3], Step [38400/41412], Loss: 2.4198, Perplexity: 11.2440\n",
      "Epoch [1/3], Step [38500/41412], Loss: 2.8218, Perplexity: 16.8069\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.3091, Perplexity: 10.0649\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.3204, Perplexity: 10.1800\n",
      "Epoch [1/3], Step [38800/41412], Loss: 2.4070, Perplexity: 11.10092\n",
      "Epoch [1/3], Step [38900/41412], Loss: 2.0393, Perplexity: 7.68533\n",
      "Epoch [1/3], Step [39000/41412], Loss: 1.9271, Perplexity: 6.86983\n",
      "Epoch [1/3], Step [39100/41412], Loss: 2.4122, Perplexity: 11.1590\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.6911, Perplexity: 14.7480\n",
      "Epoch [1/3], Step [39300/41412], Loss: 1.7912, Perplexity: 5.99659\n",
      "Epoch [1/3], Step [39400/41412], Loss: 1.6518, Perplexity: 5.21633\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.4142, Perplexity: 11.18100\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.3480, Perplexity: 10.4649\n",
      "Epoch [1/3], Step [39700/41412], Loss: 1.9887, Perplexity: 7.30610\n",
      "Epoch [1/3], Step [39800/41412], Loss: 2.0484, Perplexity: 7.75537\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.8444, Perplexity: 17.1916\n",
      "Epoch [1/3], Step [40000/41412], Loss: 2.9301, Perplexity: 18.7292\n",
      "Epoch [1/3], Step [40100/41412], Loss: 1.8906, Perplexity: 6.62337\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.0567, Perplexity: 7.82018\n",
      "Epoch [1/3], Step [40300/41412], Loss: 1.9635, Perplexity: 7.12457\n",
      "Epoch [1/3], Step [40400/41412], Loss: 1.6488, Perplexity: 5.20066\n",
      "Epoch [1/3], Step [40500/41412], Loss: 1.5186, Perplexity: 4.56602\n",
      "Epoch [1/3], Step [40600/41412], Loss: 2.6669, Perplexity: 14.3957\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.5254, Perplexity: 12.4965\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.3716, Perplexity: 10.7143\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.3066, Perplexity: 10.0398\n",
      "Epoch [1/3], Step [41000/41412], Loss: 2.6709, Perplexity: 14.4528\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.7744, Perplexity: 16.0289\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.7174, Perplexity: 15.1412\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.1338, Perplexity: 8.44719\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.5625, Perplexity: 12.9683\n",
      "Epoch [2/3], Step [100/41412], Loss: 2.2091, Perplexity: 9.1078307\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.0139, Perplexity: 7.49263\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.6324, Perplexity: 13.9078\n",
      "Epoch [2/3], Step [400/41412], Loss: 1.7083, Perplexity: 5.51985\n",
      "Epoch [2/3], Step [500/41412], Loss: 2.9702, Perplexity: 19.4956\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.3190, Perplexity: 10.1653\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.3140, Perplexity: 10.1151\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.5721, Perplexity: 13.0938\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.3387, Perplexity: 10.3674\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.3575, Perplexity: 10.5643\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.4342, Perplexity: 11.4070\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.1692, Perplexity: 8.75117\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.2831, Perplexity: 9.80704\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.2385, Perplexity: 9.37960\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.2653, Perplexity: 9.63350\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.4256, Perplexity: 11.3088\n",
      "Epoch [2/3], Step [1700/41412], Loss: 2.1279, Perplexity: 8.397485\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.1442, Perplexity: 8.53508\n",
      "Epoch [2/3], Step [1900/41412], Loss: 2.2409, Perplexity: 9.40164\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.0254, Perplexity: 7.57946\n",
      "Epoch [2/3], Step [2100/41412], Loss: 2.6588, Perplexity: 14.2792\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.6212, Perplexity: 13.7523\n",
      "Epoch [2/3], Step [2300/41412], Loss: 2.1669, Perplexity: 8.73075\n",
      "Epoch [2/3], Step [2400/41412], Loss: 1.7698, Perplexity: 5.86953\n",
      "Epoch [2/3], Step [2500/41412], Loss: 2.3383, Perplexity: 10.3637\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.6438, Perplexity: 14.0659\n",
      "Epoch [2/3], Step [2700/41412], Loss: 2.3063, Perplexity: 10.0377\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.4632, Perplexity: 11.7419\n",
      "Epoch [2/3], Step [2900/41412], Loss: 1.9682, Perplexity: 7.15740\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.4397, Perplexity: 11.4698\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.5362, Perplexity: 12.6311\n",
      "Epoch [2/3], Step [3200/41412], Loss: 2.4573, Perplexity: 11.6736\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.5138, Perplexity: 12.3513\n",
      "Epoch [2/3], Step [3400/41412], Loss: 3.0910, Perplexity: 21.9983\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.8450, Perplexity: 17.2013\n",
      "Epoch [2/3], Step [3600/41412], Loss: 1.7533, Perplexity: 5.77385\n",
      "Epoch [2/3], Step [3700/41412], Loss: 2.2901, Perplexity: 9.87622\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.5088, Perplexity: 12.2903\n",
      "Epoch [2/3], Step [3900/41412], Loss: 2.5627, Perplexity: 12.9703\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.1423, Perplexity: 8.51918\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.4200, Perplexity: 11.2459\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.6497, Perplexity: 14.1504\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.2434, Perplexity: 9.42541\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.6586, Perplexity: 14.2769\n",
      "Epoch [2/3], Step [4500/41412], Loss: 3.0715, Perplexity: 21.5742\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.0037, Perplexity: 7.41615\n",
      "Epoch [2/3], Step [4700/41412], Loss: 2.7964, Perplexity: 16.3849\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.6710, Perplexity: 14.4544\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.6240, Perplexity: 13.7903\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.5273, Perplexity: 12.52012\n",
      "Epoch [2/3], Step [5100/41412], Loss: 2.5036, Perplexity: 12.2259\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.8924, Perplexity: 18.0360\n",
      "Epoch [2/3], Step [5300/41412], Loss: 3.6137, Perplexity: 37.1025\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.3458, Perplexity: 10.4418\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.5434, Perplexity: 12.7235\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.5150, Perplexity: 12.3661\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.2588, Perplexity: 9.57179\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.6912, Perplexity: 14.7495\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.7728, Perplexity: 16.0035\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.6202, Perplexity: 13.7382\n",
      "Epoch [2/3], Step [6100/41412], Loss: 1.5584, Perplexity: 4.75103\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.3372, Perplexity: 10.3517\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.5596, Perplexity: 12.9311\n",
      "Epoch [2/3], Step [6400/41412], Loss: 1.6039, Perplexity: 4.97237\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.4681, Perplexity: 11.7995\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.3585, Perplexity: 10.5753\n",
      "Epoch [2/3], Step [6700/41412], Loss: 1.7359, Perplexity: 5.67404\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.6839, Perplexity: 14.6418\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.0278, Perplexity: 7.59773\n",
      "Epoch [2/3], Step [7000/41412], Loss: 2.1735, Perplexity: 8.78892\n",
      "Epoch [2/3], Step [7100/41412], Loss: 1.9324, Perplexity: 6.90625\n",
      "Epoch [2/3], Step [7200/41412], Loss: 2.6332, Perplexity: 13.9186\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.4826, Perplexity: 11.9723\n",
      "Epoch [2/3], Step [7400/41412], Loss: 1.8638, Perplexity: 6.44811\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.4300, Perplexity: 11.3586\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.1298, Perplexity: 8.41289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7700/41412], Loss: 2.1407, Perplexity: 8.50588\n",
      "Epoch [2/3], Step [7800/41412], Loss: 3.2529, Perplexity: 25.8652\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.2000, Perplexity: 9.02473\n",
      "Epoch [2/3], Step [8000/41412], Loss: 3.0050, Perplexity: 20.1853\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.0834, Perplexity: 8.03210\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.0110, Perplexity: 7.47072\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.1530, Perplexity: 8.61052\n",
      "Epoch [2/3], Step [8400/41412], Loss: 2.3524, Perplexity: 10.5112\n",
      "Epoch [2/3], Step [8500/41412], Loss: 2.5748, Perplexity: 13.1281\n",
      "Epoch [2/3], Step [8600/41412], Loss: 1.6823, Perplexity: 5.37800\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.0588, Perplexity: 7.83699\n",
      "Epoch [2/3], Step [8800/41412], Loss: 2.4487, Perplexity: 11.5738\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.3648, Perplexity: 10.6421\n",
      "Epoch [2/3], Step [9000/41412], Loss: 2.0271, Perplexity: 7.59188\n",
      "Epoch [2/3], Step [9100/41412], Loss: 2.1438, Perplexity: 8.53187\n",
      "Epoch [2/3], Step [9200/41412], Loss: 3.2087, Perplexity: 24.7461\n",
      "Epoch [2/3], Step [9300/41412], Loss: 2.9144, Perplexity: 18.4374\n",
      "Epoch [2/3], Step [9400/41412], Loss: 5.0021, Perplexity: 148.7220\n",
      "Epoch [2/3], Step [9500/41412], Loss: 1.8569, Perplexity: 6.40397\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.0809, Perplexity: 8.01180\n",
      "Epoch [2/3], Step [9700/41412], Loss: 2.3017, Perplexity: 9.99090\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.3840, Perplexity: 10.8486\n",
      "Epoch [2/3], Step [9900/41412], Loss: 3.0122, Perplexity: 20.3319\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.5283, Perplexity: 12.5326\n",
      "Epoch [2/3], Step [10100/41412], Loss: 1.9229, Perplexity: 6.84082\n",
      "Epoch [2/3], Step [10200/41412], Loss: 2.7685, Perplexity: 15.9353\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.0902, Perplexity: 8.08658\n",
      "Epoch [2/3], Step [10400/41412], Loss: 2.1714, Perplexity: 8.77069\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.6168, Perplexity: 13.6922\n",
      "Epoch [2/3], Step [10600/41412], Loss: 1.7782, Perplexity: 5.91924\n",
      "Epoch [2/3], Step [10700/41412], Loss: 2.4490, Perplexity: 11.5772\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.3382, Perplexity: 10.3630\n",
      "Epoch [2/3], Step [10900/41412], Loss: 2.3525, Perplexity: 10.5120\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.0283, Perplexity: 7.60093\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.2183, Perplexity: 9.19216\n",
      "Epoch [2/3], Step [11200/41412], Loss: 1.8522, Perplexity: 6.37404\n",
      "Epoch [2/3], Step [11300/41412], Loss: 2.3733, Perplexity: 10.7329\n",
      "Epoch [2/3], Step [11400/41412], Loss: 1.9250, Perplexity: 6.85555\n",
      "Epoch [2/3], Step [11500/41412], Loss: 2.3338, Perplexity: 10.3172\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.4018, Perplexity: 11.0429\n",
      "Epoch [2/3], Step [11700/41412], Loss: 2.1489, Perplexity: 8.57576\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.7753, Perplexity: 16.0429\n",
      "Epoch [2/3], Step [11900/41412], Loss: 2.8205, Perplexity: 16.7861\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.3946, Perplexity: 10.9633\n",
      "Epoch [2/3], Step [12100/41412], Loss: 2.3325, Perplexity: 10.3032\n",
      "Epoch [2/3], Step [12200/41412], Loss: 1.9566, Perplexity: 7.07537\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.1843, Perplexity: 8.88485\n",
      "Epoch [2/3], Step [12400/41412], Loss: 1.8298, Perplexity: 6.23293\n",
      "Epoch [2/3], Step [12500/41412], Loss: 2.7082, Perplexity: 15.0020\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.2046, Perplexity: 9.06644\n",
      "Epoch [2/3], Step [12700/41412], Loss: 1.9247, Perplexity: 6.85287\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.2410, Perplexity: 9.40275\n",
      "Epoch [2/3], Step [12900/41412], Loss: 1.9640, Perplexity: 7.12759\n",
      "Epoch [2/3], Step [13000/41412], Loss: 1.8876, Perplexity: 6.60356\n",
      "Epoch [2/3], Step [13100/41412], Loss: 1.9503, Perplexity: 7.03084\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.6070, Perplexity: 13.5583\n",
      "Epoch [2/3], Step [13300/41412], Loss: 1.7383, Perplexity: 5.68763\n",
      "Epoch [2/3], Step [13400/41412], Loss: 2.1042, Perplexity: 8.20081\n",
      "Epoch [2/3], Step [13500/41412], Loss: 1.9845, Perplexity: 7.27518\n",
      "Epoch [2/3], Step [13600/41412], Loss: 3.1833, Perplexity: 24.1274\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.8626, Perplexity: 17.5062\n",
      "Epoch [2/3], Step [13800/41412], Loss: 2.7111, Perplexity: 15.0454\n",
      "Epoch [2/3], Step [13900/41412], Loss: 1.9563, Perplexity: 7.07334\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.0851, Perplexity: 8.04548\n",
      "Epoch [2/3], Step [14100/41412], Loss: 1.7920, Perplexity: 6.00176\n",
      "Epoch [2/3], Step [14200/41412], Loss: 2.0888, Perplexity: 8.07526\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.7235, Perplexity: 15.2340\n",
      "Epoch [2/3], Step [14400/41412], Loss: 2.6830, Perplexity: 14.6288\n",
      "Epoch [2/3], Step [14500/41412], Loss: 2.0751, Perplexity: 7.96520\n",
      "Epoch [2/3], Step [14600/41412], Loss: 1.9677, Perplexity: 7.15411\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.6044, Perplexity: 13.5226\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.7283, Perplexity: 15.3067\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.3295, Perplexity: 10.2727\n",
      "Epoch [2/3], Step [15000/41412], Loss: 2.2449, Perplexity: 9.43936\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.0270, Perplexity: 7.59108\n",
      "Epoch [2/3], Step [15200/41412], Loss: 1.6683, Perplexity: 5.30307\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.2282, Perplexity: 9.28339\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.6572, Perplexity: 14.2557\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.4462, Perplexity: 11.5439\n",
      "Epoch [2/3], Step [15600/41412], Loss: 3.1863, Perplexity: 24.1981\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.3750, Perplexity: 10.7505\n",
      "Epoch [2/3], Step [15800/41412], Loss: 3.1291, Perplexity: 22.8533\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.0096, Perplexity: 7.46036\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.0168, Perplexity: 7.51421\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.0796, Perplexity: 8.00149\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.3307, Perplexity: 10.2849\n",
      "Epoch [2/3], Step [16300/41412], Loss: 2.2497, Perplexity: 9.48478\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.1679, Perplexity: 8.74027\n",
      "Epoch [2/3], Step [16500/41412], Loss: 2.3842, Perplexity: 10.8505\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.2849, Perplexity: 9.82450\n",
      "Epoch [2/3], Step [16700/41412], Loss: 1.5530, Perplexity: 4.72588\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.3395, Perplexity: 10.37646\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.7562, Perplexity: 15.7400\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.5317, Perplexity: 12.5750\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.4479, Perplexity: 11.5639\n",
      "Epoch [2/3], Step [17200/41412], Loss: 1.9181, Perplexity: 6.80793\n",
      "Epoch [2/3], Step [17300/41412], Loss: 1.9623, Perplexity: 7.11558\n",
      "Epoch [2/3], Step [17400/41412], Loss: 2.2551, Perplexity: 9.53627\n",
      "Epoch [2/3], Step [17500/41412], Loss: 2.1745, Perplexity: 8.79760\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.1396, Perplexity: 8.49622\n",
      "Epoch [2/3], Step [17700/41412], Loss: 2.4092, Perplexity: 11.1255\n",
      "Epoch [2/3], Step [17800/41412], Loss: 1.9708, Perplexity: 7.17636\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.4031, Perplexity: 11.0569\n",
      "Epoch [2/3], Step [18000/41412], Loss: 2.2551, Perplexity: 9.536686\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.6131, Perplexity: 13.6415\n",
      "Epoch [2/3], Step [18200/41412], Loss: 1.6902, Perplexity: 5.42083\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.9017, Perplexity: 18.2057\n",
      "Epoch [2/3], Step [18400/41412], Loss: 1.9439, Perplexity: 6.98587\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.2309, Perplexity: 9.30856\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.6647, Perplexity: 14.3641\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.6555, Perplexity: 14.2319\n",
      "Epoch [2/3], Step [18800/41412], Loss: 2.2267, Perplexity: 9.26880\n",
      "Epoch [2/3], Step [18900/41412], Loss: 1.9808, Perplexity: 7.24870\n",
      "Epoch [2/3], Step [19000/41412], Loss: 1.9402, Perplexity: 6.95990\n",
      "Epoch [2/3], Step [19100/41412], Loss: 1.9662, Perplexity: 7.14373\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.5691, Perplexity: 13.0544\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.4481, Perplexity: 11.5663\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.7222, Perplexity: 15.2140\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.7263, Perplexity: 15.2768\n",
      "Epoch [2/3], Step [19600/41412], Loss: 2.2170, Perplexity: 9.17953\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.8863, Perplexity: 17.9272\n",
      "Epoch [2/3], Step [19800/41412], Loss: 1.9278, Perplexity: 6.87447\n",
      "Epoch [2/3], Step [19900/41412], Loss: 2.6743, Perplexity: 14.5019\n",
      "Epoch [2/3], Step [20000/41412], Loss: 2.6687, Perplexity: 14.4211\n",
      "Epoch [2/3], Step [20100/41412], Loss: 2.1925, Perplexity: 8.95771\n",
      "Epoch [2/3], Step [20200/41412], Loss: 2.5036, Perplexity: 12.2259\n",
      "Epoch [2/3], Step [20300/41412], Loss: 1.7954, Perplexity: 6.02173\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.5598, Perplexity: 12.9333\n",
      "Epoch [2/3], Step [20500/41412], Loss: 2.3901, Perplexity: 10.9148\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.2831, Perplexity: 9.80650\n",
      "Epoch [2/3], Step [20700/41412], Loss: 3.1777, Perplexity: 23.9914\n",
      "Epoch [2/3], Step [20800/41412], Loss: 2.4182, Perplexity: 11.2252\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.0555, Perplexity: 7.81057\n",
      "Epoch [2/3], Step [21000/41412], Loss: 1.8759, Perplexity: 6.52668\n",
      "Epoch [2/3], Step [21100/41412], Loss: 1.9694, Perplexity: 7.16615\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.3970, Perplexity: 10.9897\n",
      "Epoch [2/3], Step [21300/41412], Loss: 2.2803, Perplexity: 9.77996\n",
      "Epoch [2/3], Step [21400/41412], Loss: 1.7971, Perplexity: 6.03190\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.6227, Perplexity: 13.7728\n",
      "Epoch [2/3], Step [21600/41412], Loss: 2.0225, Perplexity: 7.55681\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.4346, Perplexity: 11.4107\n",
      "Epoch [2/3], Step [21800/41412], Loss: 3.2323, Perplexity: 25.3382\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.7743, Perplexity: 16.0281\n",
      "Epoch [2/3], Step [22000/41412], Loss: 2.5656, Perplexity: 13.0081\n",
      "Epoch [2/3], Step [22100/41412], Loss: 2.1343, Perplexity: 8.45138\n",
      "Epoch [2/3], Step [22200/41412], Loss: 1.6412, Perplexity: 5.16133\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.5841, Perplexity: 13.2508\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.4304, Perplexity: 11.3631\n",
      "Epoch [2/3], Step [22500/41412], Loss: 3.1194, Perplexity: 22.6318\n",
      "Epoch [2/3], Step [22600/41412], Loss: 2.3778, Perplexity: 10.7816\n",
      "Epoch [2/3], Step [22700/41412], Loss: 2.5786, Perplexity: 13.1785\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.9165, Perplexity: 18.4764\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.0605, Perplexity: 7.85005\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.1540, Perplexity: 8.61958\n",
      "Epoch [2/3], Step [23100/41412], Loss: 2.0973, Perplexity: 8.14404\n",
      "Epoch [2/3], Step [23200/41412], Loss: 1.4017, Perplexity: 4.06219\n",
      "Epoch [2/3], Step [23300/41412], Loss: 2.3410, Perplexity: 10.3915\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.2519, Perplexity: 9.50616\n",
      "Epoch [2/3], Step [23500/41412], Loss: 2.6742, Perplexity: 14.5014\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.1053, Perplexity: 8.20986\n",
      "Epoch [2/3], Step [23700/41412], Loss: 2.3550, Perplexity: 10.5377\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.6354, Perplexity: 13.9491\n",
      "Epoch [2/3], Step [23900/41412], Loss: 1.8235, Perplexity: 6.19361\n",
      "Epoch [2/3], Step [24000/41412], Loss: 2.7322, Perplexity: 15.3667\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.5325, Perplexity: 12.5848\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.4023, Perplexity: 11.0488\n",
      "Epoch [2/3], Step [24300/41412], Loss: 2.6402, Perplexity: 14.0157\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.3310, Perplexity: 10.2882\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.4920, Perplexity: 12.0854\n",
      "Epoch [2/3], Step [24600/41412], Loss: 1.9977, Perplexity: 7.37235\n",
      "Epoch [2/3], Step [24700/41412], Loss: 1.9919, Perplexity: 7.32920\n",
      "Epoch [2/3], Step [24800/41412], Loss: 2.3002, Perplexity: 9.97604\n",
      "Epoch [2/3], Step [24900/41412], Loss: 2.6656, Perplexity: 14.3766\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.4285, Perplexity: 11.3415\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.2636, Perplexity: 9.61752\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.2474, Perplexity: 9.46323\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.3760, Perplexity: 10.7615\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.9156, Perplexity: 18.4601\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.5315, Perplexity: 12.5721\n",
      "Epoch [2/3], Step [25600/41412], Loss: 2.0473, Perplexity: 7.74733\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.1650, Perplexity: 8.71460\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.4651, Perplexity: 11.7652\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.0072, Perplexity: 7.44229\n",
      "Epoch [2/3], Step [26000/41412], Loss: 2.3474, Perplexity: 10.4586\n",
      "Epoch [2/3], Step [26100/41412], Loss: 2.1509, Perplexity: 8.59242\n",
      "Epoch [2/3], Step [26200/41412], Loss: 1.9655, Perplexity: 7.13881\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.7271, Perplexity: 15.2880\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.9450, Perplexity: 19.0106\n",
      "Epoch [2/3], Step [26500/41412], Loss: 2.5121, Perplexity: 12.3307\n",
      "Epoch [2/3], Step [26600/41412], Loss: 1.9400, Perplexity: 6.95888\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.0707, Perplexity: 7.93034\n",
      "Epoch [2/3], Step [26800/41412], Loss: 1.7956, Perplexity: 6.023249\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.6126, Perplexity: 13.63431\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.1639, Perplexity: 8.70517\n",
      "Epoch [2/3], Step [27100/41412], Loss: 2.0760, Perplexity: 7.97253\n",
      "Epoch [2/3], Step [27200/41412], Loss: 1.9379, Perplexity: 6.94428\n",
      "Epoch [2/3], Step [27300/41412], Loss: 1.8481, Perplexity: 6.34800\n",
      "Epoch [2/3], Step [27400/41412], Loss: 2.3362, Perplexity: 10.3423\n",
      "Epoch [2/3], Step [27500/41412], Loss: 2.3033, Perplexity: 10.0074\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.5828, Perplexity: 13.2337\n",
      "Epoch [2/3], Step [27700/41412], Loss: 2.9702, Perplexity: 19.4959\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.0207, Perplexity: 7.54338\n",
      "Epoch [2/3], Step [27900/41412], Loss: 2.3489, Perplexity: 10.4744\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.7656, Perplexity: 15.8887\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.2086, Perplexity: 9.10291\n",
      "Epoch [2/3], Step [28200/41412], Loss: 2.3406, Perplexity: 10.3871\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.5295, Perplexity: 12.5471\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.5101, Perplexity: 12.3065\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.3428, Perplexity: 10.4107\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.7079, Perplexity: 14.9981\n",
      "Epoch [2/3], Step [28700/41412], Loss: 2.4985, Perplexity: 12.1642\n",
      "Epoch [2/3], Step [28800/41412], Loss: 3.0724, Perplexity: 21.59318\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.0467, Perplexity: 7.74229\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.7555, Perplexity: 15.7287\n",
      "Epoch [2/3], Step [29100/41412], Loss: 2.7336, Perplexity: 15.3882\n",
      "Epoch [2/3], Step [29200/41412], Loss: 2.6018, Perplexity: 13.4876\n",
      "Epoch [2/3], Step [29300/41412], Loss: 1.8563, Perplexity: 6.40015\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.5218, Perplexity: 12.4507\n",
      "Epoch [2/3], Step [29500/41412], Loss: 2.5972, Perplexity: 13.4256\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.0794, Perplexity: 7.99986\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.5643, Perplexity: 12.9918\n",
      "Epoch [2/3], Step [29800/41412], Loss: 1.8357, Perplexity: 6.26965\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.2606, Perplexity: 9.58871\n",
      "Epoch [2/3], Step [30000/41412], Loss: 2.3905, Perplexity: 10.9192\n",
      "Epoch [2/3], Step [30100/41412], Loss: 1.8206, Perplexity: 6.17579\n",
      "Epoch [2/3], Step [30200/41412], Loss: 2.2387, Perplexity: 9.38117\n",
      "Epoch [2/3], Step [30300/41412], Loss: 2.4267, Perplexity: 11.3218\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.2963, Perplexity: 9.93770\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.3805, Perplexity: 10.8105\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.4698, Perplexity: 11.8199\n",
      "Epoch [2/3], Step [30700/41412], Loss: 1.8923, Perplexity: 6.63478\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.2690, Perplexity: 9.66990\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.7116, Perplexity: 15.0538\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.2230, Perplexity: 9.23521\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.6155, Perplexity: 13.6735\n",
      "Epoch [2/3], Step [31200/41412], Loss: 2.7753, Perplexity: 16.0435\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.1627, Perplexity: 8.69458\n",
      "Epoch [2/3], Step [31400/41412], Loss: 1.6641, Perplexity: 5.28114\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.3156, Perplexity: 10.1308\n",
      "Epoch [2/3], Step [31600/41412], Loss: 1.6188, Perplexity: 5.04713\n",
      "Epoch [2/3], Step [31700/41412], Loss: 2.2486, Perplexity: 9.474919\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.2058, Perplexity: 9.07759\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.3188, Perplexity: 10.1638\n",
      "Epoch [2/3], Step [32000/41412], Loss: 2.2174, Perplexity: 9.18324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [32100/41412], Loss: 2.1915, Perplexity: 8.94906\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.4853, Perplexity: 12.0046\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.4333, Perplexity: 11.3969\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.5846, Perplexity: 13.2585\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.0989, Perplexity: 8.15730\n",
      "Epoch [2/3], Step [32600/41412], Loss: 1.7900, Perplexity: 5.98974\n",
      "Epoch [2/3], Step [32700/41412], Loss: 1.9198, Perplexity: 6.81982\n",
      "Epoch [2/3], Step [32800/41412], Loss: 2.2576, Perplexity: 9.56056\n",
      "Epoch [2/3], Step [32900/41412], Loss: 1.7513, Perplexity: 5.76241\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.4162, Perplexity: 11.2029\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.2315, Perplexity: 9.31405\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.3572, Perplexity: 10.5617\n",
      "Epoch [2/3], Step [33300/41412], Loss: 1.7038, Perplexity: 5.49487\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.4623, Perplexity: 11.7318\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.3876, Perplexity: 10.8876\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.7930, Perplexity: 16.3297\n",
      "Epoch [2/3], Step [33700/41412], Loss: 1.6514, Perplexity: 5.21407\n",
      "Epoch [2/3], Step [33800/41412], Loss: 2.3691, Perplexity: 10.6876\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.0694, Perplexity: 7.92045\n",
      "Epoch [2/3], Step [34000/41412], Loss: 1.8584, Perplexity: 6.41344\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.4412, Perplexity: 11.4866\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.1845, Perplexity: 8.88620\n",
      "Epoch [2/3], Step [34300/41412], Loss: 2.4448, Perplexity: 11.5278\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.3073, Perplexity: 10.0468\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.3565, Perplexity: 10.5536\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.1016, Perplexity: 8.17927\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.6010, Perplexity: 13.4772\n",
      "Epoch [2/3], Step [34800/41412], Loss: 2.1618, Perplexity: 8.68645\n",
      "Epoch [2/3], Step [34900/41412], Loss: 2.0242, Perplexity: 7.57017\n",
      "Epoch [2/3], Step [35000/41412], Loss: 1.8785, Perplexity: 6.54394\n",
      "Epoch [2/3], Step [35100/41412], Loss: 1.6729, Perplexity: 5.32768\n",
      "Epoch [2/3], Step [35200/41412], Loss: 2.4938, Perplexity: 12.1073\n",
      "Epoch [2/3], Step [35300/41412], Loss: 1.8116, Perplexity: 6.12033\n",
      "Epoch [2/3], Step [35400/41412], Loss: 2.3495, Perplexity: 10.4802\n",
      "Epoch [2/3], Step [35500/41412], Loss: 2.1343, Perplexity: 8.45130\n",
      "Epoch [2/3], Step [35600/41412], Loss: 3.0090, Perplexity: 20.2667\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.5226, Perplexity: 12.4605\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.0129, Perplexity: 7.48524\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.9816, Perplexity: 19.7194\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.3743, Perplexity: 10.7435\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.8958, Perplexity: 18.0975\n",
      "Epoch [2/3], Step [36200/41412], Loss: 2.3750, Perplexity: 10.7514\n",
      "Epoch [2/3], Step [36300/41412], Loss: 2.0668, Perplexity: 7.89922\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.0669, Perplexity: 7.90007\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.2392, Perplexity: 9.38593\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.0522, Perplexity: 7.78491\n",
      "Epoch [2/3], Step [36700/41412], Loss: 2.3412, Perplexity: 10.3938\n",
      "Epoch [2/3], Step [36800/41412], Loss: 2.2482, Perplexity: 9.47048\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.5523, Perplexity: 12.8366\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.7398, Perplexity: 15.4842\n",
      "Epoch [2/3], Step [37100/41412], Loss: 1.6033, Perplexity: 4.96968\n",
      "Epoch [2/3], Step [37200/41412], Loss: 2.5252, Perplexity: 12.4930\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.2424, Perplexity: 9.41553\n",
      "Epoch [2/3], Step [37400/41412], Loss: 1.6306, Perplexity: 5.10718\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.0947, Perplexity: 8.12291\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.0158, Perplexity: 7.50668\n",
      "Epoch [2/3], Step [37700/41412], Loss: 2.1973, Perplexity: 9.00114\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.3195, Perplexity: 10.1707\n",
      "Epoch [2/3], Step [37900/41412], Loss: 1.9560, Perplexity: 7.07083\n",
      "Epoch [2/3], Step [38000/41412], Loss: 3.3114, Perplexity: 27.4247\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.3267, Perplexity: 10.2446\n",
      "Epoch [2/3], Step [38200/41412], Loss: 2.2245, Perplexity: 9.24878\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.3592, Perplexity: 10.5821\n",
      "Epoch [2/3], Step [38400/41412], Loss: 1.9900, Perplexity: 7.31532\n",
      "Epoch [2/3], Step [38500/41412], Loss: 1.8147, Perplexity: 6.13944\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.7848, Perplexity: 16.1971\n",
      "Epoch [2/3], Step [38700/41412], Loss: 1.9056, Perplexity: 6.72366\n",
      "Epoch [2/3], Step [38800/41412], Loss: 2.7006, Perplexity: 14.88943\n",
      "Epoch [2/3], Step [38900/41412], Loss: 1.9215, Perplexity: 6.83113\n",
      "Epoch [2/3], Step [39000/41412], Loss: 1.8921, Perplexity: 6.63320\n",
      "Epoch [2/3], Step [39100/41412], Loss: 2.0607, Perplexity: 7.85153\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.2253, Perplexity: 9.25660\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.6583, Perplexity: 14.2717\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.1822, Perplexity: 8.86587\n",
      "Epoch [2/3], Step [39500/41412], Loss: 2.2465, Perplexity: 9.45505\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.0884, Perplexity: 8.07193\n",
      "Epoch [2/3], Step [39700/41412], Loss: 2.7738, Perplexity: 16.0191\n",
      "Epoch [2/3], Step [39800/41412], Loss: 1.6221, Perplexity: 5.06356\n",
      "Epoch [2/3], Step [39900/41412], Loss: 2.2539, Perplexity: 9.52509\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.5958, Perplexity: 13.4075\n",
      "Epoch [2/3], Step [40100/41412], Loss: 2.4205, Perplexity: 11.25188\n",
      "Epoch [2/3], Step [40200/41412], Loss: 2.6125, Perplexity: 13.6326\n",
      "Epoch [2/3], Step [40300/41412], Loss: 1.9635, Perplexity: 7.12462\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.5416, Perplexity: 12.7003\n",
      "Epoch [2/3], Step [40500/41412], Loss: 2.1737, Perplexity: 8.79106\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.0799, Perplexity: 8.00389\n",
      "Epoch [2/3], Step [40700/41412], Loss: 1.9192, Perplexity: 6.815631\n",
      "Epoch [2/3], Step [40800/41412], Loss: 2.3764, Perplexity: 10.7660\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.7489, Perplexity: 15.6250\n",
      "Epoch [2/3], Step [41000/41412], Loss: 1.7513, Perplexity: 5.76213\n",
      "Epoch [2/3], Step [41100/41412], Loss: 2.3552, Perplexity: 10.5400\n",
      "Epoch [2/3], Step [41200/41412], Loss: 2.1413, Perplexity: 8.51023\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.3791, Perplexity: 10.7949\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.8130, Perplexity: 16.6591\n",
      "Epoch [3/3], Step [100/41412], Loss: 2.5586, Perplexity: 12.917739\n",
      "Epoch [3/3], Step [200/41412], Loss: 3.0444, Perplexity: 20.9978\n",
      "Epoch [3/3], Step [300/41412], Loss: 2.5410, Perplexity: 12.6928\n",
      "Epoch [3/3], Step [400/41412], Loss: 2.6703, Perplexity: 14.4443\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.3538, Perplexity: 10.5255\n",
      "Epoch [3/3], Step [600/41412], Loss: 2.4777, Perplexity: 11.9140\n",
      "Epoch [3/3], Step [700/41412], Loss: 1.7697, Perplexity: 5.86903\n",
      "Epoch [3/3], Step [800/41412], Loss: 1.9865, Perplexity: 7.28999\n",
      "Epoch [3/3], Step [900/41412], Loss: 3.0036, Perplexity: 20.1588\n",
      "Epoch [3/3], Step [1000/41412], Loss: 1.7565, Perplexity: 5.7923\n",
      "Epoch [3/3], Step [1100/41412], Loss: 2.0711, Perplexity: 7.93384\n",
      "Epoch [3/3], Step [1200/41412], Loss: 1.9525, Perplexity: 7.04629\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.3282, Perplexity: 10.2591\n",
      "Epoch [3/3], Step [1400/41412], Loss: 2.5112, Perplexity: 12.3198\n",
      "Epoch [3/3], Step [1500/41412], Loss: 1.7127, Perplexity: 5.54389\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.0555, Perplexity: 7.81115\n",
      "Epoch [3/3], Step [1700/41412], Loss: 2.2321, Perplexity: 9.31962\n",
      "Epoch [3/3], Step [1800/41412], Loss: 1.9708, Perplexity: 7.17614\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.3663, Perplexity: 10.6576\n",
      "Epoch [3/3], Step [2000/41412], Loss: 2.1456, Perplexity: 8.54762\n",
      "Epoch [3/3], Step [2100/41412], Loss: 2.9753, Perplexity: 19.5960\n",
      "Epoch [3/3], Step [2200/41412], Loss: 2.1212, Perplexity: 8.34098\n",
      "Epoch [3/3], Step [2300/41412], Loss: 1.9934, Perplexity: 7.34046\n",
      "Epoch [3/3], Step [2400/41412], Loss: 2.1184, Perplexity: 8.31766\n",
      "Epoch [3/3], Step [2500/41412], Loss: 2.4472, Perplexity: 11.5556\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.5299, Perplexity: 12.5523\n",
      "Epoch [3/3], Step [2700/41412], Loss: 2.2568, Perplexity: 9.55289\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.7135, Perplexity: 15.0823\n",
      "Epoch [3/3], Step [2900/41412], Loss: 1.8760, Perplexity: 6.52715\n",
      "Epoch [3/3], Step [3000/41412], Loss: 2.6526, Perplexity: 14.1912\n",
      "Epoch [3/3], Step [3100/41412], Loss: 2.3688, Perplexity: 10.6844\n",
      "Epoch [3/3], Step [3200/41412], Loss: 2.1884, Perplexity: 8.92102\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.0957, Perplexity: 8.13115\n",
      "Epoch [3/3], Step [3400/41412], Loss: 2.2613, Perplexity: 9.59591\n",
      "Epoch [3/3], Step [3500/41412], Loss: 2.1789, Perplexity: 8.83634\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.2885, Perplexity: 9.86068\n",
      "Epoch [3/3], Step [3700/41412], Loss: 1.8819, Perplexity: 6.56589\n",
      "Epoch [3/3], Step [3800/41412], Loss: 2.1629, Perplexity: 8.695986\n",
      "Epoch [3/3], Step [3900/41412], Loss: 1.8009, Perplexity: 6.05528\n",
      "Epoch [3/3], Step [4000/41412], Loss: 2.8227, Perplexity: 16.8225\n",
      "Epoch [3/3], Step [4100/41412], Loss: 2.0391, Perplexity: 7.68393\n",
      "Epoch [3/3], Step [4200/41412], Loss: 1.4521, Perplexity: 4.27195\n",
      "Epoch [3/3], Step [4300/41412], Loss: 2.5790, Perplexity: 13.1833\n",
      "Epoch [3/3], Step [4400/41412], Loss: 2.3536, Perplexity: 10.5235\n",
      "Epoch [3/3], Step [4500/41412], Loss: 1.9365, Perplexity: 6.93449\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.1405, Perplexity: 8.50384\n",
      "Epoch [3/3], Step [4700/41412], Loss: 2.4398, Perplexity: 11.4706\n",
      "Epoch [3/3], Step [4800/41412], Loss: 1.9300, Perplexity: 6.88971\n",
      "Epoch [3/3], Step [4900/41412], Loss: 1.6344, Perplexity: 5.12628\n",
      "Epoch [3/3], Step [5000/41412], Loss: 1.7132, Perplexity: 5.54667\n",
      "Epoch [3/3], Step [5100/41412], Loss: 1.8221, Perplexity: 6.18499\n",
      "Epoch [3/3], Step [5200/41412], Loss: 1.7528, Perplexity: 5.77065\n",
      "Epoch [3/3], Step [5300/41412], Loss: 1.9080, Perplexity: 6.73959\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.1629, Perplexity: 8.69662\n",
      "Epoch [3/3], Step [5500/41412], Loss: 2.1938, Perplexity: 8.96936\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.1317, Perplexity: 8.42928\n",
      "Epoch [3/3], Step [5700/41412], Loss: 2.8425, Perplexity: 17.1590\n",
      "Epoch [3/3], Step [5800/41412], Loss: 1.9709, Perplexity: 7.17717\n",
      "Epoch [3/3], Step [5900/41412], Loss: 2.6064, Perplexity: 13.5497\n",
      "Epoch [3/3], Step [6000/41412], Loss: 2.2010, Perplexity: 9.03433\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.4140, Perplexity: 11.1789\n",
      "Epoch [3/3], Step [6200/41412], Loss: 1.8820, Perplexity: 6.56636\n",
      "Epoch [3/3], Step [6300/41412], Loss: 1.7870, Perplexity: 5.97156\n",
      "Epoch [3/3], Step [6400/41412], Loss: 2.3202, Perplexity: 10.1778\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.2413, Perplexity: 9.40607\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.3743, Perplexity: 10.7435\n",
      "Epoch [3/3], Step [6700/41412], Loss: 2.1930, Perplexity: 8.96254\n",
      "Epoch [3/3], Step [6800/41412], Loss: 2.5329, Perplexity: 12.5900\n",
      "Epoch [3/3], Step [6900/41412], Loss: 2.0804, Perplexity: 8.00793\n",
      "Epoch [3/3], Step [7000/41412], Loss: 2.8589, Perplexity: 17.4424\n",
      "Epoch [3/3], Step [7100/41412], Loss: 1.9218, Perplexity: 6.83304\n",
      "Epoch [3/3], Step [7200/41412], Loss: 2.1542, Perplexity: 8.62082\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.5386, Perplexity: 12.6624\n",
      "Epoch [3/3], Step [7400/41412], Loss: 1.9149, Perplexity: 6.78645\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.1975, Perplexity: 9.00230\n",
      "Epoch [3/3], Step [7600/41412], Loss: 2.3551, Perplexity: 10.5390\n",
      "Epoch [3/3], Step [7700/41412], Loss: 2.5624, Perplexity: 12.9663\n",
      "Epoch [3/3], Step [7800/41412], Loss: 1.8954, Perplexity: 6.65504\n",
      "Epoch [3/3], Step [7900/41412], Loss: 1.7626, Perplexity: 5.82764\n",
      "Epoch [3/3], Step [8000/41412], Loss: 1.8394, Perplexity: 6.29265\n",
      "Epoch [3/3], Step [8100/41412], Loss: 1.6549, Perplexity: 5.23289\n",
      "Epoch [3/3], Step [8200/41412], Loss: 2.2885, Perplexity: 9.86057\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.4257, Perplexity: 11.3101\n",
      "Epoch [3/3], Step [8400/41412], Loss: 2.7030, Perplexity: 14.9250\n",
      "Epoch [3/3], Step [8500/41412], Loss: 1.6844, Perplexity: 5.38925\n",
      "Epoch [3/3], Step [8600/41412], Loss: 2.2590, Perplexity: 9.57317\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.1832, Perplexity: 8.87437\n",
      "Epoch [3/3], Step [8800/41412], Loss: 3.2506, Perplexity: 25.8053\n",
      "Epoch [3/3], Step [8900/41412], Loss: 1.7004, Perplexity: 5.47621\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.1124, Perplexity: 8.26804\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.5022, Perplexity: 12.2092\n",
      "Epoch [3/3], Step [9200/41412], Loss: 2.1784, Perplexity: 8.83202\n",
      "Epoch [3/3], Step [9300/41412], Loss: 2.5232, Perplexity: 12.4684\n",
      "Epoch [3/3], Step [9400/41412], Loss: 2.3522, Perplexity: 10.5083\n",
      "Epoch [3/3], Step [9500/41412], Loss: 2.3694, Perplexity: 10.6914\n",
      "Epoch [3/3], Step [9600/41412], Loss: 1.6614, Perplexity: 5.26661\n",
      "Epoch [3/3], Step [9700/41412], Loss: 1.5405, Perplexity: 4.66704\n",
      "Epoch [3/3], Step [9800/41412], Loss: 2.3249, Perplexity: 10.2254\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.7655, Perplexity: 15.8877\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.0840, Perplexity: 8.0363\n",
      "Epoch [3/3], Step [10100/41412], Loss: 2.5245, Perplexity: 12.4845\n",
      "Epoch [3/3], Step [10200/41412], Loss: 1.6321, Perplexity: 5.11444\n",
      "Epoch [3/3], Step [10300/41412], Loss: 3.4691, Perplexity: 32.1092\n",
      "Epoch [3/3], Step [10400/41412], Loss: 2.1982, Perplexity: 9.00852\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.3528, Perplexity: 10.5154\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.1108, Perplexity: 8.25528\n",
      "Epoch [3/3], Step [10700/41412], Loss: 2.2371, Perplexity: 9.36618\n",
      "Epoch [3/3], Step [10800/41412], Loss: 2.1160, Perplexity: 8.29809\n",
      "Epoch [3/3], Step [10900/41412], Loss: 3.3875, Perplexity: 29.5916\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.1667, Perplexity: 8.72966\n",
      "Epoch [3/3], Step [11100/41412], Loss: 1.9275, Perplexity: 6.87259\n",
      "Epoch [3/3], Step [11200/41412], Loss: 2.3027, Perplexity: 10.0009\n",
      "Epoch [3/3], Step [11300/41412], Loss: 1.9929, Perplexity: 7.33681\n",
      "Epoch [3/3], Step [11400/41412], Loss: 2.0546, Perplexity: 7.80379\n",
      "Epoch [3/3], Step [11500/41412], Loss: 1.9120, Perplexity: 6.76683\n",
      "Epoch [3/3], Step [11600/41412], Loss: 2.3580, Perplexity: 10.5696\n",
      "Epoch [3/3], Step [11700/41412], Loss: 1.9567, Perplexity: 7.07592\n",
      "Epoch [3/3], Step [11800/41412], Loss: 2.0154, Perplexity: 7.50365\n",
      "Epoch [3/3], Step [11900/41412], Loss: 1.5627, Perplexity: 4.77151\n",
      "Epoch [3/3], Step [12000/41412], Loss: 2.5693, Perplexity: 13.0570\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.3894, Perplexity: 10.9067\n",
      "Epoch [3/3], Step [12200/41412], Loss: 1.8319, Perplexity: 6.24557\n",
      "Epoch [3/3], Step [12300/41412], Loss: 1.8571, Perplexity: 6.40547\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.1303, Perplexity: 8.41737\n",
      "Epoch [3/3], Step [12500/41412], Loss: 2.7805, Perplexity: 16.1277\n",
      "Epoch [3/3], Step [12600/41412], Loss: 3.5994, Perplexity: 36.5762\n",
      "Epoch [3/3], Step [12700/41412], Loss: 2.2961, Perplexity: 9.93578\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.0746, Perplexity: 7.96141\n",
      "Epoch [3/3], Step [12900/41412], Loss: 2.0285, Perplexity: 7.60244\n",
      "Epoch [3/3], Step [13000/41412], Loss: 1.8139, Perplexity: 6.13403\n",
      "Epoch [3/3], Step [13100/41412], Loss: 1.9913, Perplexity: 7.32497\n",
      "Epoch [3/3], Step [13200/41412], Loss: 2.0770, Perplexity: 7.98025\n",
      "Epoch [3/3], Step [13300/41412], Loss: 2.1917, Perplexity: 8.95020\n",
      "Epoch [3/3], Step [13400/41412], Loss: 2.0175, Perplexity: 7.51966\n",
      "Epoch [3/3], Step [13500/41412], Loss: 1.7389, Perplexity: 5.69119\n",
      "Epoch [3/3], Step [13600/41412], Loss: 2.8127, Perplexity: 16.6547\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.1905, Perplexity: 8.93986\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.3224, Perplexity: 10.2000\n",
      "Epoch [3/3], Step [13900/41412], Loss: 1.9483, Perplexity: 7.01692\n",
      "Epoch [3/3], Step [14000/41412], Loss: 1.8794, Perplexity: 6.54978\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.0663, Perplexity: 7.89592\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.5695, Perplexity: 13.0588\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.3118, Perplexity: 10.0928\n",
      "Epoch [3/3], Step [14400/41412], Loss: 1.9524, Perplexity: 7.04531\n",
      "Epoch [3/3], Step [14500/41412], Loss: 2.2358, Perplexity: 9.35423\n",
      "Epoch [3/3], Step [14600/41412], Loss: 1.9178, Perplexity: 6.80620\n",
      "Epoch [3/3], Step [14700/41412], Loss: 2.1766, Perplexity: 8.81628\n",
      "Epoch [3/3], Step [14800/41412], Loss: 1.8849, Perplexity: 6.58585\n",
      "Epoch [3/3], Step [14900/41412], Loss: 2.0026, Perplexity: 7.40811\n",
      "Epoch [3/3], Step [15000/41412], Loss: 2.5049, Perplexity: 12.2428\n",
      "Epoch [3/3], Step [15100/41412], Loss: 2.7209, Perplexity: 15.1946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [15200/41412], Loss: 2.6300, Perplexity: 13.8738\n",
      "Epoch [3/3], Step [15300/41412], Loss: 1.5398, Perplexity: 4.66367\n",
      "Epoch [3/3], Step [15400/41412], Loss: 2.3941, Perplexity: 10.9581\n",
      "Epoch [3/3], Step [15500/41412], Loss: 2.5298, Perplexity: 12.5516\n",
      "Epoch [3/3], Step [15600/41412], Loss: 2.6900, Perplexity: 14.7316\n",
      "Epoch [3/3], Step [15700/41412], Loss: 2.5771, Perplexity: 13.1586\n",
      "Epoch [3/3], Step [15800/41412], Loss: 2.1969, Perplexity: 8.99680\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.6481, Perplexity: 14.1271\n",
      "Epoch [3/3], Step [16000/41412], Loss: 1.8789, Perplexity: 6.546471\n",
      "Epoch [3/3], Step [16100/41412], Loss: 1.9358, Perplexity: 6.92957\n",
      "Epoch [3/3], Step [16200/41412], Loss: 2.2462, Perplexity: 9.45139\n",
      "Epoch [3/3], Step [16300/41412], Loss: 1.7797, Perplexity: 5.92811\n",
      "Epoch [3/3], Step [16400/41412], Loss: 1.9333, Perplexity: 6.91237\n",
      "Epoch [3/3], Step [16500/41412], Loss: 1.9836, Perplexity: 7.26888\n",
      "Epoch [3/3], Step [16600/41412], Loss: 1.8847, Perplexity: 6.58440\n",
      "Epoch [3/3], Step [16700/41412], Loss: 2.2553, Perplexity: 9.53786\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.5087, Perplexity: 12.2887\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.6063, Perplexity: 13.5487\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.4311, Perplexity: 11.3714\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.4800, Perplexity: 11.9410\n",
      "Epoch [3/3], Step [17200/41412], Loss: 2.3200, Perplexity: 10.1753\n",
      "Epoch [3/3], Step [17300/41412], Loss: 2.5385, Perplexity: 12.6603\n",
      "Epoch [3/3], Step [17400/41412], Loss: 1.8648, Perplexity: 6.45448\n",
      "Epoch [3/3], Step [17500/41412], Loss: 1.9884, Perplexity: 7.30429\n",
      "Epoch [3/3], Step [17600/41412], Loss: 1.7626, Perplexity: 5.82734\n",
      "Epoch [3/3], Step [17700/41412], Loss: 1.8642, Perplexity: 6.45095\n",
      "Epoch [3/3], Step [17800/41412], Loss: 2.3176, Perplexity: 10.1515\n",
      "Epoch [3/3], Step [17900/41412], Loss: 2.1456, Perplexity: 8.54756\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.2094, Perplexity: 9.11043\n",
      "Epoch [3/3], Step [18100/41412], Loss: 2.7381, Perplexity: 15.4574\n",
      "Epoch [3/3], Step [18200/41412], Loss: 1.6894, Perplexity: 5.41645\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.9506, Perplexity: 19.1171\n",
      "Epoch [3/3], Step [18400/41412], Loss: 2.1497, Perplexity: 8.58240\n",
      "Epoch [3/3], Step [18500/41412], Loss: 2.5182, Perplexity: 12.4068\n",
      "Epoch [3/3], Step [18600/41412], Loss: 2.0334, Perplexity: 7.64044\n",
      "Epoch [3/3], Step [18700/41412], Loss: 2.9636, Perplexity: 19.3675\n",
      "Epoch [3/3], Step [18800/41412], Loss: 2.0755, Perplexity: 7.96848\n",
      "Epoch [3/3], Step [18900/41412], Loss: 2.1875, Perplexity: 8.91306\n",
      "Epoch [3/3], Step [19000/41412], Loss: 1.9818, Perplexity: 7.25574\n",
      "Epoch [3/3], Step [19100/41412], Loss: 2.4266, Perplexity: 11.3207\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.4790, Perplexity: 11.9297\n",
      "Epoch [3/3], Step [19300/41412], Loss: 3.0449, Perplexity: 21.0081\n",
      "Epoch [3/3], Step [19400/41412], Loss: 1.9535, Perplexity: 7.05332\n",
      "Epoch [3/3], Step [19500/41412], Loss: 1.4354, Perplexity: 4.20136\n",
      "Epoch [3/3], Step [19600/41412], Loss: 2.6650, Perplexity: 14.3676\n",
      "Epoch [3/3], Step [19700/41412], Loss: 1.8273, Perplexity: 6.21714\n",
      "Epoch [3/3], Step [19800/41412], Loss: 1.8745, Perplexity: 6.51730\n",
      "Epoch [3/3], Step [19900/41412], Loss: 2.4435, Perplexity: 11.5129\n",
      "Epoch [3/3], Step [20100/41412], Loss: 3.0988, Perplexity: 22.1716\n",
      "Epoch [3/3], Step [20200/41412], Loss: 1.7936, Perplexity: 6.01113\n",
      "Epoch [3/3], Step [20300/41412], Loss: 2.2270, Perplexity: 9.27169\n",
      "Epoch [3/3], Step [20400/41412], Loss: 2.2840, Perplexity: 9.81573\n",
      "Epoch [3/3], Step [20500/41412], Loss: 1.8848, Perplexity: 6.58537\n",
      "Epoch [3/3], Step [20600/41412], Loss: 2.1805, Perplexity: 8.85055\n",
      "Epoch [3/3], Step [20700/41412], Loss: 1.5688, Perplexity: 4.80092\n",
      "Epoch [3/3], Step [20800/41412], Loss: 1.8878, Perplexity: 6.60473\n",
      "Epoch [3/3], Step [20900/41412], Loss: 2.0695, Perplexity: 7.92050\n",
      "Epoch [3/3], Step [21000/41412], Loss: 2.3960, Perplexity: 10.9797\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.3332, Perplexity: 10.3106\n",
      "Epoch [3/3], Step [21200/41412], Loss: 1.9787, Perplexity: 7.23314\n",
      "Epoch [3/3], Step [21300/41412], Loss: 3.3756, Perplexity: 29.2427\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.1096, Perplexity: 8.24505\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.3987, Perplexity: 11.0094\n",
      "Epoch [3/3], Step [21600/41412], Loss: 1.8379, Perplexity: 6.28338\n",
      "Epoch [3/3], Step [21700/41412], Loss: 2.1325, Perplexity: 8.43572\n",
      "Epoch [3/3], Step [21800/41412], Loss: 1.6712, Perplexity: 5.31841\n",
      "Epoch [3/3], Step [21900/41412], Loss: 2.1745, Perplexity: 8.79806\n",
      "Epoch [3/3], Step [22000/41412], Loss: 1.9770, Perplexity: 7.22074\n",
      "Epoch [3/3], Step [22100/41412], Loss: 2.0299, Perplexity: 7.61310\n",
      "Epoch [3/3], Step [22200/41412], Loss: 2.1579, Perplexity: 8.65250\n",
      "Epoch [3/3], Step [22300/41412], Loss: 1.8371, Perplexity: 6.27813\n",
      "Epoch [3/3], Step [22400/41412], Loss: 2.2260, Perplexity: 9.262826\n",
      "Epoch [3/3], Step [22500/41412], Loss: 1.8305, Perplexity: 6.23717\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.2706, Perplexity: 9.68524\n",
      "Epoch [3/3], Step [22700/41412], Loss: 2.1551, Perplexity: 8.62924\n",
      "Epoch [3/3], Step [22800/41412], Loss: 1.9501, Perplexity: 7.02928\n",
      "Epoch [3/3], Step [22900/41412], Loss: 2.0244, Perplexity: 7.57153\n",
      "Epoch [3/3], Step [23000/41412], Loss: 2.9986, Perplexity: 20.0578\n",
      "Epoch [3/3], Step [23100/41412], Loss: 2.5818, Perplexity: 13.2208\n",
      "Epoch [3/3], Step [23200/41412], Loss: 2.4554, Perplexity: 11.6513\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.0753, Perplexity: 7.96716\n",
      "Epoch [3/3], Step [23400/41412], Loss: 2.0528, Perplexity: 7.78975\n",
      "Epoch [3/3], Step [23500/41412], Loss: 1.8882, Perplexity: 6.60722\n",
      "Epoch [3/3], Step [23600/41412], Loss: 3.0001, Perplexity: 20.0877\n",
      "Epoch [3/3], Step [23700/41412], Loss: 2.5134, Perplexity: 12.3467\n",
      "Epoch [3/3], Step [23800/41412], Loss: 2.5423, Perplexity: 12.7094\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.4706, Perplexity: 11.8297\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.2021, Perplexity: 9.04400\n",
      "Epoch [3/3], Step [24100/41412], Loss: 2.5307, Perplexity: 12.5629\n",
      "Epoch [3/3], Step [24200/41412], Loss: 2.1150, Perplexity: 8.28994\n",
      "Epoch [3/3], Step [24300/41412], Loss: 2.2996, Perplexity: 9.97049\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.2376, Perplexity: 9.37064\n",
      "Epoch [3/3], Step [24500/41412], Loss: 2.1225, Perplexity: 8.35211\n",
      "Epoch [3/3], Step [24600/41412], Loss: 1.9093, Perplexity: 6.74840\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.3229, Perplexity: 10.2053\n",
      "Epoch [3/3], Step [24800/41412], Loss: 2.0338, Perplexity: 7.64314\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.2877, Perplexity: 9.85206\n",
      "Epoch [3/3], Step [25000/41412], Loss: 2.0642, Perplexity: 7.87893\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.0561, Perplexity: 7.81578\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.9815, Perplexity: 19.7182\n",
      "Epoch [3/3], Step [25300/41412], Loss: 2.1365, Perplexity: 8.46930\n",
      "Epoch [3/3], Step [25400/41412], Loss: 2.2355, Perplexity: 9.35123\n",
      "Epoch [3/3], Step [25500/41412], Loss: 2.6786, Perplexity: 14.5646\n",
      "Epoch [3/3], Step [25600/41412], Loss: 1.7665, Perplexity: 5.85062\n",
      "Epoch [3/3], Step [25700/41412], Loss: 2.0368, Perplexity: 7.66606\n",
      "Epoch [3/3], Step [25800/41412], Loss: 2.7758, Perplexity: 16.0508\n",
      "Epoch [3/3], Step [25900/41412], Loss: 1.8549, Perplexity: 6.39127\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.0533, Perplexity: 7.79398\n",
      "Epoch [3/3], Step [26100/41412], Loss: 2.0690, Perplexity: 7.91672\n",
      "Epoch [3/3], Step [26200/41412], Loss: 2.0808, Perplexity: 8.01110\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.5695, Perplexity: 13.0599\n",
      "Epoch [3/3], Step [26400/41412], Loss: 2.4066, Perplexity: 11.0963\n",
      "Epoch [3/3], Step [26500/41412], Loss: 1.8429, Perplexity: 6.31488\n",
      "Epoch [3/3], Step [26600/41412], Loss: 1.9325, Perplexity: 6.90692\n",
      "Epoch [3/3], Step [26700/41412], Loss: 2.1091, Perplexity: 8.24123\n",
      "Epoch [3/3], Step [26800/41412], Loss: 1.8718, Perplexity: 6.50005\n",
      "Epoch [3/3], Step [26900/41412], Loss: 2.5088, Perplexity: 12.2901\n",
      "Epoch [3/3], Step [27000/41412], Loss: 2.5469, Perplexity: 12.7680\n",
      "Epoch [3/3], Step [27100/41412], Loss: 2.0204, Perplexity: 7.54131\n",
      "Epoch [3/3], Step [27200/41412], Loss: 2.7494, Perplexity: 15.6333\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.7639, Perplexity: 15.8623\n",
      "Epoch [3/3], Step [27400/41412], Loss: 2.5265, Perplexity: 12.5101\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.0521, Perplexity: 7.78412\n",
      "Epoch [3/3], Step [27600/41412], Loss: 1.8807, Perplexity: 6.55798\n",
      "Epoch [3/3], Step [27700/41412], Loss: 2.2820, Perplexity: 9.79642\n",
      "Epoch [3/3], Step [27800/41412], Loss: 1.8795, Perplexity: 6.55047\n",
      "Epoch [3/3], Step [27900/41412], Loss: 1.9733, Perplexity: 7.19477\n",
      "Epoch [3/3], Step [28000/41412], Loss: 1.9631, Perplexity: 7.12131\n",
      "Epoch [3/3], Step [28100/41412], Loss: 2.0348, Perplexity: 7.65114\n",
      "Epoch [3/3], Step [28200/41412], Loss: 2.0119, Perplexity: 7.47722\n",
      "Epoch [3/3], Step [28300/41412], Loss: 2.0681, Perplexity: 7.91015\n",
      "Epoch [3/3], Step [28400/41412], Loss: 2.9288, Perplexity: 18.7055\n",
      "Epoch [3/3], Step [28500/41412], Loss: 2.2266, Perplexity: 9.26854\n",
      "Epoch [3/3], Step [28600/41412], Loss: 2.6607, Perplexity: 14.3064\n",
      "Epoch [3/3], Step [28700/41412], Loss: 2.5519, Perplexity: 12.8313\n",
      "Epoch [3/3], Step [28800/41412], Loss: 2.3386, Perplexity: 10.3664\n",
      "Epoch [3/3], Step [28900/41412], Loss: 2.0736, Perplexity: 7.95330\n",
      "Epoch [3/3], Step [29000/41412], Loss: 2.1560, Perplexity: 8.63645\n",
      "Epoch [3/3], Step [29100/41412], Loss: 2.3951, Perplexity: 10.9689\n",
      "Epoch [3/3], Step [29200/41412], Loss: 2.0555, Perplexity: 7.81042\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.4215, Perplexity: 11.2631\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.5670, Perplexity: 13.0267\n",
      "Epoch [3/3], Step [29500/41412], Loss: 1.5113, Perplexity: 4.53284\n",
      "Epoch [3/3], Step [29600/41412], Loss: 2.2755, Perplexity: 9.73274\n",
      "Epoch [3/3], Step [29700/41412], Loss: 2.4332, Perplexity: 11.3956\n",
      "Epoch [3/3], Step [29800/41412], Loss: 1.9500, Perplexity: 7.02853\n",
      "Epoch [3/3], Step [29900/41412], Loss: 1.8966, Perplexity: 6.66347\n",
      "Epoch [3/3], Step [30000/41412], Loss: 2.5953, Perplexity: 13.4008\n",
      "Epoch [3/3], Step [30100/41412], Loss: 2.1764, Perplexity: 8.81430\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.3994, Perplexity: 11.0167\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.0318, Perplexity: 7.62810\n",
      "Epoch [3/3], Step [30400/41412], Loss: 1.4391, Perplexity: 4.21699\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.0782, Perplexity: 7.99043\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.7854, Perplexity: 16.2059\n",
      "Epoch [3/3], Step [30700/41412], Loss: 2.4766, Perplexity: 11.9009\n",
      "Epoch [3/3], Step [30800/41412], Loss: 1.7480, Perplexity: 5.74292\n",
      "Epoch [3/3], Step [30900/41412], Loss: 2.0262, Perplexity: 7.58544\n",
      "Epoch [3/3], Step [31000/41412], Loss: 2.3373, Perplexity: 10.3536\n",
      "Epoch [3/3], Step [31100/41412], Loss: 1.7825, Perplexity: 5.94485\n",
      "Epoch [3/3], Step [31200/41412], Loss: 2.0774, Perplexity: 7.98386\n",
      "Epoch [3/3], Step [31300/41412], Loss: 1.9161, Perplexity: 6.79432\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.0928, Perplexity: 8.10784\n",
      "Epoch [3/3], Step [31500/41412], Loss: 1.9094, Perplexity: 6.74919\n",
      "Epoch [3/3], Step [31600/41412], Loss: 2.4147, Perplexity: 11.1864\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.2351, Perplexity: 9.34797\n",
      "Epoch [3/3], Step [31800/41412], Loss: 2.6829, Perplexity: 14.6276\n",
      "Epoch [3/3], Step [31900/41412], Loss: 2.0754, Perplexity: 7.96744\n",
      "Epoch [3/3], Step [32000/41412], Loss: 1.9018, Perplexity: 6.69785\n",
      "Epoch [3/3], Step [32100/41412], Loss: 2.4786, Perplexity: 11.9248\n",
      "Epoch [3/3], Step [32200/41412], Loss: 3.1647, Perplexity: 23.6821\n",
      "Epoch [3/3], Step [32300/41412], Loss: 2.4805, Perplexity: 11.9475\n",
      "Epoch [3/3], Step [32400/41412], Loss: 2.3697, Perplexity: 10.6946\n",
      "Epoch [3/3], Step [32500/41412], Loss: 2.1042, Perplexity: 8.20022\n",
      "Epoch [3/3], Step [32600/41412], Loss: 2.0618, Perplexity: 7.86018\n",
      "Epoch [3/3], Step [32700/41412], Loss: 2.0035, Perplexity: 7.41523\n",
      "Epoch [3/3], Step [32800/41412], Loss: 2.4172, Perplexity: 11.2143\n",
      "Epoch [3/3], Step [32900/41412], Loss: 1.8103, Perplexity: 6.11228\n",
      "Epoch [3/3], Step [33000/41412], Loss: 2.4868, Perplexity: 12.0233\n",
      "Epoch [3/3], Step [33100/41412], Loss: 2.5012, Perplexity: 12.1970\n",
      "Epoch [3/3], Step [33200/41412], Loss: 3.0206, Perplexity: 20.5036\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.1080, Perplexity: 8.23144\n",
      "Epoch [3/3], Step [33400/41412], Loss: 1.8324, Perplexity: 6.24891\n",
      "Epoch [3/3], Step [33500/41412], Loss: 2.4041, Perplexity: 11.0687\n",
      "Epoch [3/3], Step [33600/41412], Loss: 2.5629, Perplexity: 12.9740\n",
      "Epoch [3/3], Step [33700/41412], Loss: 2.0896, Perplexity: 8.08153\n",
      "Epoch [3/3], Step [33800/41412], Loss: 2.1493, Perplexity: 8.57918\n",
      "Epoch [3/3], Step [33900/41412], Loss: 1.9167, Perplexity: 6.79828\n",
      "Epoch [3/3], Step [34000/41412], Loss: 1.8110, Perplexity: 6.11681\n",
      "Epoch [3/3], Step [34100/41412], Loss: 2.1483, Perplexity: 8.57061\n",
      "Epoch [3/3], Step [34200/41412], Loss: 2.6190, Perplexity: 13.7222\n",
      "Epoch [3/3], Step [34300/41412], Loss: 2.2149, Perplexity: 9.16013\n",
      "Epoch [3/3], Step [34400/41412], Loss: 1.9465, Perplexity: 7.00390\n",
      "Epoch [3/3], Step [34500/41412], Loss: 2.1914, Perplexity: 8.94759\n",
      "Epoch [3/3], Step [34600/41412], Loss: 1.7652, Perplexity: 5.84261\n",
      "Epoch [3/3], Step [34700/41412], Loss: 2.8795, Perplexity: 17.8052\n",
      "Epoch [3/3], Step [34800/41412], Loss: 1.6595, Perplexity: 5.25672\n",
      "Epoch [3/3], Step [34900/41412], Loss: 1.8734, Perplexity: 6.51050\n",
      "Epoch [3/3], Step [35000/41412], Loss: 2.5428, Perplexity: 12.7147\n",
      "Epoch [3/3], Step [35100/41412], Loss: 2.7319, Perplexity: 15.3627\n",
      "Epoch [3/3], Step [35200/41412], Loss: 2.5815, Perplexity: 13.2169\n",
      "Epoch [3/3], Step [35300/41412], Loss: 2.4645, Perplexity: 11.7575\n",
      "Epoch [3/3], Step [35400/41412], Loss: 1.9655, Perplexity: 7.13825\n",
      "Epoch [3/3], Step [35500/41412], Loss: 2.5637, Perplexity: 12.9844\n",
      "Epoch [3/3], Step [35600/41412], Loss: 2.5684, Perplexity: 13.0449\n",
      "Epoch [3/3], Step [35700/41412], Loss: 1.9214, Perplexity: 6.83081\n",
      "Epoch [3/3], Step [35800/41412], Loss: 2.1200, Perplexity: 8.33091\n",
      "Epoch [3/3], Step [35900/41412], Loss: 2.2955, Perplexity: 9.92992\n",
      "Epoch [3/3], Step [36000/41412], Loss: 2.1991, Perplexity: 9.01654\n",
      "Epoch [3/3], Step [36100/41412], Loss: 1.7984, Perplexity: 6.03978\n",
      "Epoch [3/3], Step [36200/41412], Loss: 2.3734, Perplexity: 10.7341\n",
      "Epoch [3/3], Step [36300/41412], Loss: 2.5260, Perplexity: 12.5039\n",
      "Epoch [3/3], Step [36400/41412], Loss: 1.6375, Perplexity: 5.14256\n",
      "Epoch [3/3], Step [36500/41412], Loss: 1.9940, Perplexity: 7.34485\n",
      "Epoch [3/3], Step [36600/41412], Loss: 2.2468, Perplexity: 9.45747\n",
      "Epoch [3/3], Step [36700/41412], Loss: 2.5886, Perplexity: 13.3114\n",
      "Epoch [3/3], Step [36800/41412], Loss: 3.1538, Perplexity: 23.4242\n",
      "Epoch [3/3], Step [36900/41412], Loss: 2.3585, Perplexity: 10.5747\n",
      "Epoch [3/3], Step [37000/41412], Loss: 1.7821, Perplexity: 5.94224\n",
      "Epoch [3/3], Step [37100/41412], Loss: 2.4263, Perplexity: 11.3169\n",
      "Epoch [3/3], Step [37200/41412], Loss: 2.4017, Perplexity: 11.0418\n",
      "Epoch [3/3], Step [37300/41412], Loss: 2.4449, Perplexity: 11.5297\n",
      "Epoch [3/3], Step [37400/41412], Loss: 2.2191, Perplexity: 9.19895\n",
      "Epoch [3/3], Step [37500/41412], Loss: 2.5395, Perplexity: 12.6733\n",
      "Epoch [3/3], Step [37600/41412], Loss: 1.9295, Perplexity: 6.88635\n",
      "Epoch [3/3], Step [37700/41412], Loss: 2.4407, Perplexity: 11.4812\n",
      "Epoch [3/3], Step [37800/41412], Loss: 2.1868, Perplexity: 8.90631\n",
      "Epoch [3/3], Step [37900/41412], Loss: 1.9559, Perplexity: 7.07061\n",
      "Epoch [3/3], Step [38000/41412], Loss: 2.6822, Perplexity: 14.6172\n",
      "Epoch [3/3], Step [38100/41412], Loss: 1.8840, Perplexity: 6.580089\n",
      "Epoch [3/3], Step [38200/41412], Loss: 2.5479, Perplexity: 12.7806\n",
      "Epoch [3/3], Step [38300/41412], Loss: 1.7396, Perplexity: 5.69522\n",
      "Epoch [3/3], Step [38400/41412], Loss: 2.1808, Perplexity: 8.85336\n",
      "Epoch [3/3], Step [38500/41412], Loss: 2.1032, Perplexity: 8.19228\n",
      "Epoch [3/3], Step [38600/41412], Loss: 2.0207, Perplexity: 7.54359\n",
      "Epoch [3/3], Step [38700/41412], Loss: 1.9816, Perplexity: 7.25445\n",
      "Epoch [3/3], Step [38800/41412], Loss: 2.7462, Perplexity: 15.5830\n",
      "Epoch [3/3], Step [38900/41412], Loss: 2.2708, Perplexity: 9.68705\n",
      "Epoch [3/3], Step [39000/41412], Loss: 2.2932, Perplexity: 9.90627\n",
      "Epoch [3/3], Step [39100/41412], Loss: 1.9443, Perplexity: 6.98879\n",
      "Epoch [3/3], Step [39200/41412], Loss: 2.3021, Perplexity: 9.99486\n",
      "Epoch [3/3], Step [39300/41412], Loss: 1.9233, Perplexity: 6.84371\n",
      "Epoch [3/3], Step [39400/41412], Loss: 2.6275, Perplexity: 13.8391\n",
      "Epoch [3/3], Step [39500/41412], Loss: 2.2530, Perplexity: 9.51652\n",
      "Epoch [3/3], Step [39600/41412], Loss: 3.0923, Perplexity: 22.0285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [39700/41412], Loss: 2.3080, Perplexity: 10.0548\n",
      "Epoch [3/3], Step [39800/41412], Loss: 2.3839, Perplexity: 10.8477\n",
      "Epoch [3/3], Step [39900/41412], Loss: 1.9577, Perplexity: 7.08345\n",
      "Epoch [3/3], Step [40000/41412], Loss: 2.5546, Perplexity: 12.8658\n",
      "Epoch [3/3], Step [40100/41412], Loss: 2.9772, Perplexity: 19.6334\n",
      "Epoch [3/3], Step [40200/41412], Loss: 1.4770, Perplexity: 4.37991\n",
      "Epoch [3/3], Step [40300/41412], Loss: 2.4204, Perplexity: 11.2502\n",
      "Epoch [3/3], Step [40400/41412], Loss: 2.3149, Perplexity: 10.1240\n",
      "Epoch [3/3], Step [40500/41412], Loss: 2.1221, Perplexity: 8.34887\n",
      "Epoch [3/3], Step [40600/41412], Loss: 2.5279, Perplexity: 12.5275\n",
      "Epoch [3/3], Step [40700/41412], Loss: 2.8515, Perplexity: 17.3138\n",
      "Epoch [3/3], Step [40800/41412], Loss: 2.6204, Perplexity: 13.7411\n",
      "Epoch [3/3], Step [40900/41412], Loss: 2.3294, Perplexity: 10.2723\n",
      "Epoch [3/3], Step [41000/41412], Loss: 2.1998, Perplexity: 9.02304\n",
      "Epoch [3/3], Step [41100/41412], Loss: 2.0328, Perplexity: 7.63572\n",
      "Epoch [3/3], Step [41200/41412], Loss: 2.5292, Perplexity: 12.5436\n",
      "Epoch [3/3], Step [41300/41412], Loss: 1.7984, Perplexity: 6.04012\n",
      "Epoch [3/3], Step [41400/41412], Loss: 1.9838, Perplexity: 7.27052\n",
      "Epoch [3/3], Step [41412/41412], Loss: 3.0164, Perplexity: 20.4185"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "        \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
