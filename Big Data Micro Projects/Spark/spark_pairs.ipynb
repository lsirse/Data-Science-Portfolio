{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro project\n",
    "\n",
    "## Spark: Counting the Number of Pairs \n",
    "\n",
    "We will find all the pairs of two consequent words where the first word is “narodnaya”. Then for each pair we will count the number of occurrences in the Wikipedia dump. \n",
    "\n",
    "One motivation for counting these continuations is to get a better understanding of the language. Some words, like “the”, have a lot of continuations, while others, like “San”, have just a few (“San Francisco”, for example). One can build a language model with these statistics. If you are interested to learn more, search for “n-gram language model” in the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narodnaya_gazeta\t1\n",
      "narodnaya_volya\t9\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"Pairs\").setMaster(\"local\"))\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def collect_pairs(words):\n",
    "    pairs = []\n",
    "    for i in range(len(words)):\n",
    "        if(words[i].lower() == \"narodnaya\"):\n",
    "            pairs.append((words[i].lower() + \"_\" + words[i + 1],1))\n",
    "    return pairs\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = unicode(line.rstrip()).split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        words = [element.lower() for element in words]\n",
    "        return words\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "    \n",
    "\n",
    "# Load and parallelize text file\n",
    "wiki = sc.textFile(\"articles-part\", 16)\n",
    "# Parse the article\n",
    "wiki = wiki.map(parse_article)\n",
    "# Find the pairs where the first word is narodnaya\n",
    "wiki = wiki.map(collect_pairs)\n",
    "# Count the pairs\n",
    "wiki = wiki.flatMap(lambda x: x).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Sort the pairs\n",
    "wiki = wiki.sortByKey()\n",
    "\n",
    "# Print the total\n",
    "result = wiki.collect()\n",
    "\n",
    "for t in result:\n",
    "    print(str(t[0]) + \"\\t\" + str(t[1]))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Project is part of \"Big Data Essentials: HDFS, MapReduce and Spark RDD by Yandex\" course at Coursera_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
